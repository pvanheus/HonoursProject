
@article{pennisi_will_2011,
	title = {Will Computers Crash Genomics?},
	volume = {331},
	issn = {0036-8075},
	number = {6018},
	journal = {Science},
	author = {Pennisi, E.},
	year = {2011},
	pages = {666},
	file = {Science-2011-Pennisi-666-8.pdf:/home/pvh/Documents/Papers/Workflows/Science-2011-Pennisi-666-8.pdf:application/pdf}
},

@article{yu_taxonomy_2005,
	title = {A taxonomy of workflow management systems for grid computing},
	volume = {3},
	issn = {1570-7873},
	number = {3},
	journal = {Journal of Grid Computing},
	author = {Yu, J and Buyya, R},
	year = {2005},
	keywords = {grid, workflow},
	pages = {171--200},
	file = {GridWorkflowTaxonomy.pdf:/home/pvh/Documents/Papers/GridWorkflowTaxonomy.pdf:application/pdf;WorkflowTaxonomy-Arxiv.pdf:/home/pvh/Documents/Papers/WorkflowTaxonomy-Arxiv.pdf:application/pdf}
},

@article{specht_proteomics_????,
	title = {Proteomics to go: Proteomatic enables the user-friendly creation of versatile {MS/MS} data evaluation workflows},
	shorttitle = {Proteomics to go},
	url = {http://bioinformatics.oxfordjournals.org/content/early/2011/02/16/bioinformatics.btr081.abstract},
	doi = {10.1093/bioinformatics/btr081},
	abstract = {Summary: We present Proteomatic, an operating system-independent and user-friendly platform that enables the construction and execution of {MS/MS} data evaluation pipelines using free and commercial software. Required external programs such as for peptide identification are downloaded automatically in the case of free software. Due to a strict separation of functionality and presentation, and support for multiple scripting languages, new processing steps can be added {easily.Availability} and Implementation: Proteomatic is implemented in {C++/Qt}, scripts are implemented in Ruby, Python and {PHP.} All source code is released under the {LGPL.} Source code and installers for Windows, Mac {OS} X, and Linux are freely available at {http://www.proteomatic.org.Contact:} michael.specht@uni-muenster.de},
	urldate = {2011-03-17},
	journal = {Bioinformatics},
	author = {Specht, Michael and Kuhlgert, Sebastian and Fufezan, Christian and Hippler, Michael},
	file = {Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/AMKRTWXN/bioinformatics.btr081.html:text/html}
},

@article{swertz_molgenis_2010,
	title = {The {MOLGENIS} toolkit: rapid prototyping of biosoftware at the push of a button},
	volume = {11},
	issn = {1471-2105},
	shorttitle = {The {MOLGENIS} toolkit},
	url = {http://www.biomedcentral.com/1471-2105/11/S12/S12},
	doi = {10.1186/1471-2105-11-S12-S12},
	abstract = {{BACKGROUND:There} is a huge demand on bioinformaticians to provide their biologists with user friendly and scalable software infrastructures to capture, exchange, and exploit the unprecedented amounts of new *omics data. We here present {MOLGENIS}, a generic, open source, software toolkit to quickly produce the bespoke {MOLecular} {GENetics} Information Systems {needed.METHODS:The} {MOLGENIS} toolkit provides bioinformaticians with a simple language to model biological data structures and user interfaces. At the push of a button, {MOLGENIS'} generator suite automatically translates these models into a feature-rich, ready-to-use web application including database, user interfaces, exchange formats, and scriptable interfaces. Each generator is a template of {SQL}, {JAVA}, R, or {HTML} code that would require much effort to write by hand. This 'model-driven' method ensures reuse of best practices and improves quality because the modeling language and generators are shared between all {MOLGENIS} applications, so that errors are found quickly and improvements are shared easily by a re-generation. A plug-in mechanism ensures that both the generator suite and generated product can be customized just as much as hand-written {software.RESULTS:In} recent years we have successfully evaluated the {MOLGENIS} toolkit for the rapid prototyping of many types of biomedical applications, including next-generation sequencing, {GWAS}, {QTL}, proteomics and biobanking. Writing 500 lines of model {XML} typically replaces 15,000 lines of hand-written programming code, which allows for quick adaptation if the information system is not yet to the biologist's satisfaction. Each application generated with {MOLGENIS} comes with an optimized database back-end, user interfaces for biologists to manage and exploit their data, programming interfaces for bioinformaticians to script analysis tools in R, Java, {SOAP}, {REST/JSON} and {RDF}, a tab-delimited file format to ease upload and exchange of data, and detailed technical documentation. Existing databases can be quickly enhanced with {MOLGENIS} generated interfaces using the {'ExtractModel'} {procedure.CONCLUSIONS:The} {MOLGENIS} toolkit provides bioinformaticians with a simple model to quickly generate flexible web platforms for all possible genomic, molecular and phenotypic experiments with a richness of interfaces not provided by other tools. All the software and manuals are available free as {LGPLv3} open source at http://www.molgenis.org.},
	number = {Suppl 12},
	urldate = {2011-03-17},
	journal = {{BMC} Bioinformatics},
	author = {Swertz, Morris and Dijkstra, Martijn and Adamusiak, Tomasz and van der Velde, Joeri and Kanterakis, Alexandros and Roos, Erik and Lops, Joris and Thorisson, Gudmundur and Arends, Danny and Byelas, George and Muilu, Juha and Brookes, Anthony and de Brock, Engbert and Jansen, Ritsert and Parkinson, Helen},
	year = {2010},
	pages = {S12},
	file = {biomedcentral PDF:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/N39GA94F/Swertz et al. - 2010 - The MOLGENIS toolkit rapid prototyping of biosoft.pdf:application/pdf;biomedcentral Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/TRJN7MT2/S12.html:text/html}
},

@article{_molgenis:_????,
	title = {{MOLGENIS:} a open source software toolkit to rapidly generate bespoke biosoftware web applications},
	shorttitle = {{MOLGENIS}},
	url = {http://www.molgenis.org},
	urldate = {2011-03-17},
	file = {MOLGENIS:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/NXMXTERJ/www.molgenis.org.html:text/html}
},

@inproceedings{thrasher_taming_????,
	title = {Taming complex bioinformatics workflows with weaver, makeflow, and starch},
	isbn = {2151-1373},
	publisher = {{IEEE}},
	author = {Thrasher, A. and Carmichael, R. and Bui, P. and Yu, L. and Thain, D. and Emrich, S.},
	pages = {1--6}
},

@inproceedings{roure_myexperiment:_2008,
	address = {Los Alamitos, {CA}, {USA}},
	title = {{myExperiment:} Defining the Social Virtual Research Environment},
	volume = {0},
	isbn = {978-0-7695-3535-7},
	shorttitle = {{myExperiment}},
	doi = {http://doi.ieeecomputersociety.org/10.1109/eScience.2008.86},
	abstract = {The {myExperiment} Virtual Research Environment supports the sharing of research objects used by scientists, such as scientific workflows. For researchers it is both a social infrastructure that encourages sharing and a platform for conducting research, through familiar user interfaces. For developers it provides an open, extensible and participative environment. We describe the design, implementation and deployment of {myExperiment} and suggest that its four capabilities - research objects, social model, open environment and actioning research - are necessary characteristics of an effective Virtual Research Environment for e-research and open science.},
	booktitle = {{eScience}, {IEEE} International Conference on},
	publisher = {{IEEE} Computer Society},
	author = {Roure, David De and Goble, Carole and Bhagat, Jiten and Cruickshank, Don and Goderis, Antoon and Michaelides, Danius and Newman, David},
	year = {2008},
	keywords = {scientific workflow, taverna, virtual research environment, web 2.0},
	pages = {182--189},
	annote = {Complete {PDF} document was either not available or accessible. Please make sure you're logged in to the digital library to retrieve the complete {PDF} document.},
	file = {escimyexpv6.pdf:/home/pvh/Documents/Papers/Workflows/escimyexpv6.pdf:application/pdf;myExperiment: Defining the Social Virtual Research Environment:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/N3VPBPBN/eScience.2008.html:text/html}
},

@article{abouelhoda_meta-workflows:_2010,
	series = {Wands '10},
	title = {Meta-workflows: pattern-based interoperability between Galaxy and Taverna},
	location = {Indianapolis, Indiana},
	shorttitle = {Meta-workflows},
	doi = {10.1145/1833398.1833400},
	abstract = {Taverna and Galaxy are two workflow systems developed specifically for bioinformatics applications. For sequence analysis applications, some tasks can be implemented easily on one system but would be difficult, or infeasible, to be implemented on the other. One solution to overcome this situation is to combine both tools in a unified framework that seamlessly makes use of the best features of each tool. In this paper, we present the architecture and implementation of a high-level system that provides such a solution. Our approach is based on meta-workflows and workflow patterns. We present a case study about the design of universal primers to demonstrate the capabilities of our system and to explain how the interplay between Taverna and Galaxy simplifies the analysis process.},
	journal = {Proceedings of the 1st International Workshop on Workflow Approaches to New Data-centric Science},
	author = {Abouelhoda, Mohamed and Alaa, Shady and Ghanem, Moustafa},
	year = {2010},
	note = {{ACM} {ID:} 1833400},
	keywords = {algorithms, biology and genetics, design, galaxy, taverna},
	pages = {2:1–2:8},
	file = {a2-abouelhoda.pdf:/home/pvh/Documents/Papers/Workflows/a2-abouelhoda.pdf:application/pdf;ACM Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/UXA45K83/citation.html:text/html}
},

@article{goecks_galaxy:_2010,
	title = {Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences},
	volume = {11},
	shorttitle = {Galaxy},
	number = {8},
	journal = {Genome Biol},
	author = {Goecks, J. and Nekrutenko, A. and Taylor, J. and The Galaxy Team},
	year = {2010},
	keywords = {galaxy},
	file = {gb-2010-11-8-r86.pdf:/home/pvh/Documents/Papers/Workflows/gb-2010-11-8-r86.pdf:application/pdf}
},

@book{taylor_using_2002,
	title = {Using Galaxy to Perform Large-Scale Interactive Data Analyses},
	isbn = {9780471250951},
	url = {http://dx.doi.org/10.1002/0471250953.bi1005s19},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Taylor, James and Schenck, Ian and Blankenberg, Dan and Nekrutenko, Anton},
	year = {2002},
	keywords = {comparative genomics, galaxy, genomic alignments, genomic sequences, Web application},
	annote = {While most experimental biologists know where to download genomic data, few have a concrete plan on how to analyze it. This situation can be corrected by: (1) providing unified portals serving genomic data and (2) building Web applications to allow flexible retrieval and on-the-fly analyses of the data. Powerful resources, such as the {UCSC} Genome Browser already address the first issue. The second issue, however, remains open. For example, how to find human protein-coding exons with the highest density of single nucleotide polymorphisms {(SNPs)} and extract orthologous sequences from all sequenced mammals? Indeed, one can access all relevant data from the {UCSC} Genome Browser. But once the data is downloaded how would one deal with millions of {SNPs} and gigabytes of alignments? Galaxy (http://g2.bx.psu.edu) is designed specifically for that purpose. It amplifies the strengths of existing resources (such as {UCSC} Genome Browser) by allowing the user to access and, most importantly, analyze data within a single interface in an unprecedented number of ways. Curr. Protoc. Bioinform. 19:10.5.1-10.5.25. © 2007 by John Wiley \& Sons, Inc.}
},

@book{blankenberg_galaxy:_2010,
	title = {Galaxy: A Web-Based Genome Analysis Tool for Experimentalists},
	isbn = {9780471142720},
	url = {http://dx.doi.org/10.1002/0471142727.mb1910s89},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Blankenberg, Daniel and Kuster, Gregory Von and Coraor, Nathaniel and Ananda, Guruprasad and Lazarus, Ross and Mangan, Mary and Nekrutenko, Anton and Taylor, James},
	year = {2010},
	keywords = {algorithm, analysis, bioinformatics, galaxy, genomics, pipeline, {SNPs}, workflow},
	annote = {High-throughput data production has revolutionized molecular biology. However, massive increases in data generation capacity require analysis approaches that are more sophisticated, and often very computationally intensive. Thus, making sense of high-throughput data requires informatics support. Galaxy (http://galaxyproject.org) is a software system that provides this support through a framework that gives experimentalists simple interfaces to powerful tools, while automatically managing the computational details. Galaxy is distributed both as a publicly available Web service, which provides tools for the analysis of genomic, comparative genomic, and functional genomic data, or a downloadable package that can be deployed in individual laboratories. Either way, it allows experimentalists without informatics or programming expertise to perform complex large-scale analysis with just a Web browser. Curr. Protoc. Mol. Biol. 89:19.10.1-19.10.21. © 2010 by John Wiley \& Sons, Inc.},
	file = {galaxy_cpmb_2010.pdf:/home/pvh/Documents/Papers/Workflows/galaxy_cpmb_2010.pdf:application/pdf}
},

@article{mesirov_computer_2010,
	title = {Computer science. Accessible reproducible research},
	volume = {327},
	issn = {1095-9203},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/20093459},
	doi = {10.1126/science.1179653},
	number = {5964},
	urldate = {2011-03-19},
	journal = {Science {(New} York, {N.Y.)}},
	author = {Mesirov, Jill P},
	month = jan,
	year = {2010},
	note = {{PMID:} 20093459},
	keywords = {Access to Information, Computational Biology, Computing Methodologies, Genomics, Informatics, Internet, Publishing, Reproducibility of Results, Research, Software},
	pages = {415--416},
	file = {science-reproducible-research.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/DUVHK49U/science-reproducible-research.pdf:application/pdf}
},

@article{sandve_genomic_2010,
	title = {The Genomic {HyperBrowser:} inferential genomics at the sequence level},
	volume = {11},
	issn = {1465-6906},
	shorttitle = {The Genomic {HyperBrowser}},
	url = {http://genomebiology.com/2010/11/12/R121/abstract},
	doi = {10.1186/gb-2010-11-12-r121},
	number = {12},
	urldate = {2011-03-19},
	journal = {Genome Biology},
	author = {Sandve, Geir K and Gundersen, Sveinung and Rydbeck, Halfdan and Glad, Ingrid and Holden, Lars and Holden, Marit and Liestol, Knut and Clancy, Trevor and Ferkingstad, Egil and Johansen, Morten and Nygaard, Vegard and Tostesen, Eivind and Frigessi, Arnoldo and Hovig, Eivind},
	year = {2010},
	keywords = {galaxy},
	pages = {R121},
	file = {gb-2010-11-12-r121.pdf:/home/pvh/Documents/Papers/Workflows/gb-2010-11-12-r121.pdf:application/pdf;Genome Biology | Abstract | The Genomic HyperBrowser: inferential genomics at the sequence level:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/SCJKKWR2/abstract.html:text/html}
},

@article{lazarus_toward_2008,
	title = {Toward the commoditization of translational genomic research: Design and implementation features of the Galaxy genomic workbench},
	volume = {2008},
	issn = {2153-6430},
	shorttitle = {Toward the commoditization of translational genomic research},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/21347127},
	abstract = {Although there is now plenty of genomic data and no shortage of analysis methods for translational genomic research, many biologists do not have efficient and transparent access to the computational resources they need. No single data resource or analysis application is ever likely to efficiently address all aspects of any individual researcher's needs, so most researchers are forced to manually integrate data and outputs from multiple resources. The inevitable heterogeneity of data formats and of command syntax between data resources and software applications presents a major obstacle, particularly to those biologists lacking practical informatics skills. We describe some design and implementation features of an open-source application that supports the integration of the best available third-party genomics software applications, data and annotation resources into a coherent framework, substantially overcoming many practical challenges associated with actually doing translational genomic research.},
	urldate = {2011-03-19},
	journal = {Summit on Translational Bioinformatics},
	author = {Lazarus, Ross and Taylor, James and Qiu, Weiliang and Nekrutenko, Anton},
	year = {2008},
	note = {{PMID:} 21347127},
	keywords = {galaxy},
	pages = {56--60},
	file = {amia-s2008-56.pdf:/home/pvh/Documents/Papers/Workflows/amia-s2008-56.pdf:application/pdf}
},

@article{blankenberg_framework_2007,
	title = {A framework for collaborative analysis of {ENCODE} data: making large-scale analyses biologist-friendly},
	volume = {17},
	issn = {1088-9051},
	shorttitle = {A framework for collaborative analysis of {ENCODE} data},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/17568012},
	doi = {10.1101/gr.5578007},
	abstract = {The standardization and sharing of data and tools are the biggest challenges of large collaborative projects such as the Encyclopedia of {DNA} Elements {(ENCODE).} Here we describe a compact Web application, {Galaxy2(ENCODE)}, that effectively addresses these issues. It provides an intuitive interface for the deposition and access of data, and features a vast number of analysis tools including operations on genomic intervals, utilities for manipulation of multiple sequence alignments, and molecular evolution algorithms. By providing a direct link between data and analysis tools, {Galaxy2(ENCODE)} allows addressing biological questions that are beyond the reach of existing software. We use {Galaxy2(ENCODE)} to show that the {ENCODE} regions contain {\textgreater}2000 unannotated transcripts under strong purifying selection that are likely functional. We also show that the {ENCODE} regions are representative of the entire genome by estimating the rate of nucleotide substitution and comparing it to published data. Although each of these analyses is complex, none takes more than 15 min from beginning to end. Finally, we demonstrate how new tools can be added to {Galaxy2(ENCODE)} with almost no effort. Every section of the manuscript is supplemented with {QuickTime} screencasts. {Galaxy2(ENCODE)} and the screencasts can be accessed at http://g2.bx.psu.edu.},
	number = {6},
	urldate = {2011-03-19},
	journal = {Genome Research},
	author = {Blankenberg, Daniel and Taylor, James and Schenck, Ian and He, Jianbin and Zhang, Yi and Ghent, Matthew and Veeraraghavan, Narayanan and Albert, Istvan and Miller, Webb and Makova, Kateryna D and Hardison, Ross C and Nekrutenko, Anton},
	month = jun,
	year = {2007},
	note = {{PMID:} 17568012},
	keywords = {Databases, Genetic, galaxy, Genome, Human, Humans, Internet, Sequence Analysis, {DNA}, Software},
	pages = {960--964},
	file = {960.pdf:/home/pvh/Documents/Papers/Workflows/960.pdf:application/pdf}
},

@article{giardine_galaxy:_2005,
	title = {Galaxy: a platform for interactive large-scale genome analysis},
	volume = {15},
	issn = {1088-9051},
	shorttitle = {Galaxy},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/16169926},
	doi = {10.1101/gr.4086505},
	abstract = {Accessing and analyzing the exponentially expanding genomic sequence and functional data pose a challenge for biomedical researchers. Here we describe an interactive system, Galaxy, that combines the power of existing genome annotation databases with a simple Web portal to enable users to search remote resources, combine data from independent queries, and visualize the results. The heart of Galaxy is a flexible history system that stores the queries from each user; performs operations such as intersections, unions, and subtractions; and links to other computational tools. Galaxy can be accessed at http://g2.bx.psu.edu.},
	number = {10},
	urldate = {2011-03-19},
	journal = {Genome Research},
	author = {Giardine, Belinda and Riemer, Cathy and Hardison, Ross C and Burhans, Richard and Elnitski, Laura and Shah, Prachi and Zhang, Yi and Blankenberg, Daniel and Albert, Istvan and Taylor, James and Miller, Webb and Kent, W James and Nekrutenko, Anton},
	month = oct,
	year = {2005},
	note = {{PMID:} 16169926},
	keywords = {Biological Evolution, Databases, Genetic, galaxy, Genome, Internet, Promoter Regions, Genetic},
	pages = {1451--1455},
	file = {Genome Res.-2005-Giardine-1451-5.pdf:/home/pvh/Documents/Papers/Workflows/Genome Res.-2005-Giardine-1451-5.pdf:application/pdf}
},

@article{afgan_galaxy_2010,
	title = {Galaxy {CloudMan:} delivering cloud compute clusters},
	volume = {11 Suppl 12},
	issn = {1471-2105},
	shorttitle = {Galaxy {CloudMan}},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/21210983},
	doi = {10.1186/1471-2105-11-S12-S4},
	abstract = {{BACKGROUND}
Widespread adoption of high-throughput sequencing has greatly increased the scale and sophistication of computational infrastructure needed to perform genomic research. An alternative to building and maintaining local infrastructure is "cloud computing", which, in principle, offers on demand access to flexible computational infrastructure. However, cloud computing resources are not yet suitable for immediate "as is" use by experimental biologists.

{RESULTS}
We present a cloud resource management system that makes it possible for individual researchers to compose and control an arbitrarily sized compute cluster on Amazon's {EC2} cloud infrastructure without any informatics requirements. Within this system, an entire suite of biological tools packaged by the {NERC} Bio-Linux team (http://nebc.nerc.ac.uk/tools/bio-linux) is available for immediate consumption. The provided solution makes it possible, using only a web browser, to create a completely configured compute cluster ready to perform analysis in less than five minutes. Moreover, we provide an automated method for building custom deployments of cloud resources. This approach promotes reproducibility of results and, if desired, allows individuals and labs to add or customize an otherwise available cloud system to better meet their needs.

{CONCLUSIONS}
The expected knowledge and associated effort with deploying a compute cluster in the Amazon {EC2} cloud is not trivial. The solution presented in this paper eliminates these barriers, making it possible for researchers to deploy exactly the amount of computing power they need, combined with a wealth of existing analysis software, to handle the ongoing data deluge.},
	urldate = {2011-03-19},
	journal = {{BMC} Bioinformatics},
	author = {Afgan, Enis and Baker, Dannon and Coraor, Nate and Chapman, Brad and Nekrutenko, Anton and Taylor, James},
	year = {2010},
	note = {{PMID:} 21210983},
	keywords = {galaxy},
	pages = {S4},
	file = {1471-2105-11-S12-S4.pdf:/home/pvh/Documents/Papers/Workflows/1471-2105-11-S12-S4.pdf:application/pdf}
},

@article{williams_openhelix:_2010,
	title = {{OpenHelix:} bioinformatics education outside of a different box},
	volume = {11},
	shorttitle = {{OpenHelix}},
	url = {http://bib.oxfordjournals.org/content/11/6/598.abstract},
	doi = {10.1093/bib/bbq026},
	abstract = {The amount of biological data is increasing rapidly, and will continue to increase as new rapid technologies are developed. Professionals in every area of bioscience will have data management needs that require publicly available bioinformatics resources. Not all scientists desire a formal bioinformatics education but would benefit from more informal educational sources of learning. Effective bioinformatics education formats will address a broad range of scientific needs, will be aimed at a variety of user skill levels, and will be delivered in a number of different formats to address different learning styles. Informal sources of bioinformatics education that are effective are available, and will be explored in this review.},
	number = {6},
	urldate = {2011-03-19},
	journal = {Briefings in Bioinformatics},
	author = {Williams, Jennifer M. and Mangan, Mary E. and Perreault-Micale, Cynthia and Lathe, Scott and Sirohi, Neeraj and Lathe, Warren C.},
	month = nov,
	year = {2010},
	keywords = {notfinished},
	pages = {598 --609},
	file = {Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/BICJ5CI4/598.html:text/html}
},

@article{freire_provenance_2008,
	title = {Provenance for computational tasks: A survey},
	volume = {10},
	issn = {0740-7475},
	number = {3},
	journal = {Computing in Science \& Engineering},
	author = {Freire, J. and Koop, D. and Santos, E. and Silva, {C.T.}},
	year = {2008},
	pages = {11--21},
	file = {10.1.1.147.3801.pdf:/home/pvh/Documents/Papers/Workflows/10.1.1.147.3801.pdf:application/pdf;10.1.1.90.5724.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/KDR6CRAS/10.1.1.90.5724.pdf:application/pdf}
},

@article{curcin_design_2010,
	title = {The design and implementation of a workflow analysis tool},
	volume = {368},
	url = {http://rsta.royalsocietypublishing.org/content/368/1926/4193.abstract},
	doi = {10.1098/rsta.2010.0157},
	abstract = {Motivated by the use of scientific workflows as a user-oriented mechanism for building executable scientific data integration and analysis applications, this article introduces a framework and a set of associated methods for analysing the execution properties of scientific workflows. Our framework uses a number of formal modelling techniques to characterize the process and data behaviour of workflows and workflow components and to reason about their functional and execution properties. We use the framework to design the architecture of a customizable tool that can be used to analyse the key execution properties of scientific workflows at authoring stage. Our design is generic and can be applied to a wide variety of scientific workflow languages and systems, and is evaluated by building a prototype of the tool for the Discovery Net system. We demonstrate and discuss the utility of the framework and tool using workflows from a real-world medical informatics study.},
	number = {1926},
	urldate = {2011-03-19},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Curcin, Vasa and Ghanem, Moustafa and Guo, Yike},
	year = {2010},
	keywords = {workflow},
	pages = {4193 --4208},
	file = {CurcinPaper.pdf:/home/pvh/Documents/Papers/Workflows/CurcinPaper.pdf:application/pdf;Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/2UJX7TKP/4193.html:text/html}
},

@article{curcin_towards_2010,
	title = {Towards a scientific workflow methodology for primary care database studies},
	volume = {19},
	url = {http://smm.sagepub.com/content/19/4/378.abstract},
	doi = {10.1177/0962280209359880},
	abstract = {We describe the challenges of conducting studies based on mining large-scale primary care databases, namely data integration, data set definition, result reproducibility and reusability. These correspond to higher-level informatics challenges of automation, provenance capture and component integration. We provide a high-level view of the informatics infrastructure that addresses these challenges through a generic workflowbased e-Science middleware, and describe our experiences using the system to investigate differences in the health status of patients with diabetes before and after the national introduction of the {UK} {GP} contract in 2004.},
	number = {4},
	urldate = {2011-03-19},
	journal = {Statistical Methods in Medical Research},
	author = {Curcin, Vasa and Bottle, Alex and Molokhia, Mariam and Millett, Christopher and Majeed, Azeem},
	year = {2010},
	keywords = {notfinished},
	pages = {378 --393},
	file = {Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/XU6DFGHM/378.html:text/html}
},

@article{oinn_taverna:_2006,
	title = {Taverna: lessons in creating a workflow environment for the life sciences},
	volume = {18},
	issn = {1532-0634},
	url = {http://dx.doi.org/10.1002/cpe.993},
	number = {10},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Oinn, Tom and Greenwood, Mark and Addis, Matthew and Alpdemir, M. Nedim and Ferris, Justin and Glover, Kevin and Goble, Carole and Goderis, Antoon and Hull, Duncan and Marvin, Darren and Li, Peter and Lord, Phillip and Pocock, Matthew R. and Senger, Martin and Stevens, Robert and Wipat, Anil and Wroe, Chris},
	year = {2006},
	keywords = {life sciences, scientific workflow, Semantic Grid environment, taverna, Web services},
	pages = {1067--1100},
	annote = {Abstract 10.1002/cpe.993.abs Life sciences research is based on individuals, often with diverse skills, assembled into research groups. These groups use their specialist expertise to address scientific problems. The in silico experiments undertaken by these research groups can be represented as workflows involving the co-ordinated use of analysis programs and information repositories that may be globally distributed. With regards to Grid computing, the requirements relate to the sharing of analysis and information resources rather than sharing computational power. The {myGrid} project has developed the Taverna Workbench for the composition and execution of workflows for the life sciences community. This experience paper describes lessons learnt during the development of Taverna. A common theme is the importance of understanding how workflows fit into the scientists' experimental context. The lessons reflect an evolving understanding of life scientists' requirements on a workflow environment, which is relevant to other areas of data intensive and exploratory science. Copyright © 2005 John Wiley \& Sons, Ltd.},
	file = {10.1.1.91.1913.pdf:/home/pvh/Documents/Papers/Workflows/10.1.1.91.1913.pdf:application/pdf}
},

@article{liu_weblab:_2009,
	title = {{WebLab:} a data-centric, knowledge-sharing bioinformatic platform},
	volume = {37},
	issn = {0305-1048},
	number = {suppl 2},
	journal = {Nucleic Acids Research},
	author = {Liu, X. and Wu, J. and Wang, J. and Liu, X. and Zhao, S. and Li, Z. and Kong, L. and Gu, X. and Luo, J. and Gao, G.},
	year = {2009},
	pages = {W33},
	file = {gkp428.pdf:/home/pvh/Documents/Papers/Workflows/gkp428.pdf:application/pdf}
},

@article{hull_taverna:_2006,
	title = {Taverna: a tool for building and running workflows of services},
	volume = {34},
	issn = {0305-1048},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1538887/},
	number = {suppl 2},
	journal = {Nucleic acids research},
	author = {Hull, D. and Wolstencroft, K. and Stevens, R. and Goble, C. and Pocock, {M.R.} and Li, P. and Oinn, T.},
	year = {2006},
	keywords = {taverna},
	pages = {W729},
	file = {gkl320.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/SNTMMEHV/gkl320.pdf:application/pdf}
},

@article{goble_myexperiment:_2010,
	title = {{myExperiment:} a repository and social network for the sharing of bioinformatics workflows},
	volume = {38},
	issn = {0305-1048},
	number = {suppl 2},
	journal = {Nucleic acids research},
	author = {Goble, {C.A.} and Bhagat, J. and Aleksejevs, S. and Cruickshank, D. and Michaelides, D. and Newman, D. and Borkum, M. and Bechhofer, S. and Roos, M. and Li, P.},
	year = {2010},
	keywords = {taverna},
	pages = {W677},
	file = {gkq429.pdf:/home/pvh/Documents/Papers/Workflows/gkq429.pdf:application/pdf}
},

@article{oinn_taverna:_2004,
	title = {Taverna: a tool for the composition and enactment of bioinformatics workflows},
	issn = {1367-4803},
	journal = {Bioinformatics},
	author = {Oinn, T. and Addis, M. and Ferris, J. and Marvin, D. and Greenwood, M. and Carver, T. and Pocock, {M.R.} and Wipat, A. and Li, P.},
	year = {2004},
	keywords = {taverna},
	file = {652.pdf:/home/pvh/Documents/Papers/Workflows/652.pdf:application/pdf}
},

@article{barker_scientific_2008,
	series = {{PPAM'07}},
	title = {Scientific workflow: a survey and research directions},
	location = {Gdansk, Poland},
	shorttitle = {Scientific workflow},
	url = {http://portal.acm.org/citation.cfm?id=1786194.1786281},
	abstract = {Workflow technologies are emerging as the dominant approach to coordinate groups of distributed services. However with a space filled with competing specifications, standards and frameworks from multiple domains, choosing the right tool for the job is not always a straightforward task. Researchers are often unaware of the range of technology that already exists and focus on implementing yet another proprietary workflow system. As an antidote to this common problem, this paper presents a concise survey of existing workflow technology from the business and scientific domain and makes a number of key suggestions towards the future development of scientific workflow systems.},
	urldate = {2011-03-20},
	journal = {Proceedings of the 7th international conference on Parallel processing and applied mathematics},
	author = {Barker, Adam and Van Hemert, Jano},
	year = {2008},
	note = {{ACM} {ID:} 1786281},
	keywords = {workflow},
	pages = {746–753},
	file = {10.1.1.105.4605.pdf:/home/pvh/Documents/Papers/Workflows/10.1.1.105.4605.pdf:application/pdf;ACM Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/TG4QEHT2/citation.html:text/html}
},

@article{roure_software_2009,
	title = {Software Design for Empowering Scientists},
	volume = {26},
	issn = {0740-7459},
	abstract = {Scientific research is increasingly digital. Some activities, such as data analysis, search, and simulation, can be accelerated by letting scientists write workflows and scripts that automate routine activities. These capture pieces of the scientific method that scientists can share. The averna Workbench, a widely deployed scientific-workflow-management system, together with the {myExperiment} social Web site for sharing scientific experiments, follow six principles of designing software for adoption by scientists and six principles of user engagement.},
	number = {1},
	journal = {{IEEE} Software},
	author = {Roure, David De and Goble, Carole},
	year = {2009},
	keywords = {agile software development, myexperiment social web site, scientific workflow management systems, taverna, taverna workflow workbench},
	pages = {88--95},
	annote = {Complete {PDF} document was either not available or accessible. Please make sure you're logged in to the digital library to retrieve the complete {PDF} document.},
	file = {myExpSoftwareFinal.pdf:/home/pvh/Documents/Papers/Workflows/myExpSoftwareFinal.pdf:application/pdf;Software Design for Empowering Scientists:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/9Q2GX2QP/MS.2009.html:text/html}
},

@article{ludascher_scientific_2006,
	title = {Scientific workflow management and the Kepler system},
	volume = {18},
	issn = {1532-0634},
	url = {http://dx.doi.org/10.1002/cpe.994},
	number = {10},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Ludäscher, Bertram and Altintas, Ilkay and Berkley, Chad and Higgins, Dan and Jaeger, Efrat and Jones, Matthew and Lee, Edward A. and Tao, Jing and Zhao, Yang},
	year = {2006},
	keywords = {dataflow networks, Grid workflows, kepler, problem-solving environments, scientific data management, scientific workflows, workflow},
	pages = {1039--1065},
	annote = {Abstract 10.1002/cpe.994.abs Many scientific disciplines are now data and information driven, and new scientific knowledge is often gained by scientists putting together data analysis and knowledge discovery ‘pipelines’. A related trend is that more and more scientific communities realize the benefits of sharing their data and computational services, and are thus contributing to a distributed data and computational community infrastructure (a.k.a. ‘the Grid’). However, this infrastructure is only a means to an end and ideally scientists should not be too concerned with its existence. The goal is for scientists to focus on development and use of what we call scientific workflows. These are networks of analytical steps that may involve, e.g., database access and querying steps, data analysis and mining steps, and many other steps including computationally intensive jobs on high-performance cluster computers. In this paper we describe characteristics of and requirements for scientific workflows as identified in a number of our application projects. We then elaborate on Kepler, a particular scientific workflow system, currently under development across a number of scientific data management projects. We describe some key features of Kepler and its underlying Ptolemy {II} system, planned extensions, and areas of future research. Kepler is a community-driven, open source project, and we always welcome related projects and new contributors to join. Copyright © 2005 John Wiley \& Sons, Ltd.},
	file = {10.1.1.59.6066.pdf:/home/pvh/Documents/Papers/Workflows/10.1.1.59.6066.pdf:application/pdf;computer07.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/MTDWKT64/computer07.pdf:application/pdf}
},

@book{taylor_workflows_2007,
	title = {Workflows for e-science: scientific workflows for grids},
	isbn = {9781846285196},
	shorttitle = {Workflows for e-science},
	publisher = {Springer},
	author = {Taylor, Ian J.},
	year = {2007},
	keywords = {notfinished, workflow},
	file = {20923216_lese_1.pdf:/home/pvh/Documents/Papers/Workflows/20923216_lese_1.pdf:application/pdf;FP060089-00.pdf:/home/pvh/Documents/Papers/Workflows/FP060089-00.pdf:application/pdf;Generating_Complex_Astronomy_Workflows.pdf:/home/pvh/Documents/Papers/Workflows/Generating_Complex_Astronomy_Workflows.pdf:application/pdf;workflowbook_tavernachap_final.pdf:/home/pvh/Documents/Papers/Workflows/workflowbook_tavernachap_final.pdf:application/pdf}
},

@inproceedings{curcin_scientific_2008,
	title = {Scientific workflow systems-can one size fit all?},
	author = {Curcin, V. and Ghanem, M.},
	year = {2008},
	keywords = {workflow},
	pages = {1--9},
	file = {Scientific_workflow_systems.pdf:/home/pvh/Documents/Papers/Workflows/Scientific_workflow_systems.pdf:application/pdf}
},

@article{rowe_discovery_2003,
	title = {The discovery net system for high throughput bioinformatics},
	volume = {19 Suppl 1},
	issn = {1367-4803},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/12855463},
	abstract = {{MOTIVATION:} Bioinformatics requires Grid technologies and protocols to build high performance applications without focusing on the low level detail of how the individual Grid components operate. {RESULTS:} The Discovery Net system is a middleware that allows service developers to integrate tools based on existing and emerging Grid standards such as web services. Once integrated, these tools can be used to compose reusable workflows using these services that can later be deployed as new services for others to use. Using the Discovery Net system and a range of different bioinformatics tools, we built a Grid based application for Genome Annotation. This includes workflows for automatic nucleotide annotation, annotation of predicted proteins and text analysis based on metabolic profiles and text analysis.},
	urldate = {2011-03-22},
	journal = {Bioinformatics {(Oxford}, England)},
	author = {Rowe, Anthony and Kalaitzopoulos, Dimitrios and Osmond, Michelle and Ghanem, Moustafa and Guo, Yike},
	year = {2003},
	note = {{PMID:} 12855463},
	keywords = {algorithms, Computational Biology, Computing Methodologies, Databases, Bibliographic, Databases, Genetic, Documentation, Information Storage and Retrieval, Internet, Natural Language Processing, Periodicals as Topic, Software, Software Design},
	pages = {i225--231},
	file = {i225.full.pdf:/home/pvh/Documents/Papers/Workflows/i225.full.pdf:application/pdf}
},

@article{oinn_talisman--rapid_2003,
	title = {Talisman--rapid application development for the grid},
	volume = {19 Suppl 1},
	issn = {1367-4803},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/12855460},
	abstract = {In order to make use of the emerging grid and network services offered by various institutes and mandated by many current research projects, some kind of user accessible client is required. In contrast with attempts to build generic workbenches, Talisman is designed to allow a bioinformatics expert to rapidly build custom applications, immediately visible using standard web technology, for users who wish to concentrate on the biology of their problem rather than the informatics aspects. As a component of the {MyGrid} project, it is intended to allow access to arbitrary resources, including but not limited to relational, object and flat file data sources, analysis programs and grid based storage, tracking and distributed annotation systems.},
	urldate = {2011-03-22},
	journal = {Bioinformatics {(Oxford}, England)},
	author = {Oinn, Thomas M},
	year = {2003},
	note = {{PMID:} 12855460},
	keywords = {Computational Biology, Computer Systems, Computing Methodologies, Database Management Systems, Databases, Genetic, Information Storage and Retrieval, Internet, Online Systems, Software, Software Design, Systems Integration, workflow},
	pages = {i212--214},
	file = {i212.full.pdf:/home/pvh/Documents/Papers/Workflows/i212.full.pdf:application/pdf}
},

@article{gil_artificial_2004,
	title = {Artificial intelligence and grids: Workflow planning and beyond},
	volume = {19},
	issn = {1541-1672},
	number = {1},
	journal = {Intelligent Systems, {IEEE}},
	author = {Gil, Y. and Deelman, E. and Blythe, J. and Kesselman, C. and Tangmunarunkit, H.},
	year = {2004},
	pages = {26--33},
	file = {10.1.1.83.3884.pdf:/home/pvh/Documents/Papers/Workflows/10.1.1.83.3884.pdf:application/pdf}
},

@inproceedings{deelman_cost_2008,
	title = {The cost of doing science on the cloud: the montage example},
	publisher = {{IEEE} Press},
	author = {Deelman, E. and Singh, G. and Livny, M. and Berriman, B. and Good, J.},
	year = {2008},
	keywords = {cloud, costing, workflow},
	pages = {50},
	file = {ewa-ec2.pdf:/home/pvh/Documents/Papers/Workflows/ewa-ec2.pdf:application/pdf}
},

@misc{aalst_workflow_2003,
	title = {Workflow patterns},
	url = {http://library.tue.nl/csp/dare/LinkToRepository.csp?recordnumber=613310},
	urldate = {2011-03-22},
	author = {Aalst, {WMP} van der and Hofstede, {AHM} ter and Kiepuszewski, B and Barros, {AP}},
	year = {2003},
	annote = {{OAI} Repository of the Technische Universiteit {Eindhoven(TU/e)[http://cache.libr.tue.nl:1972/csp/dare/DARE.Repository.cls](Netherlands)ER}},
	file = {9950.pdf:/home/pvh/Documents/Papers/Workflows/9950.pdf:application/pdf;Full Text (HTML):/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/KMAU7XNM/LinkToVubis.html:text/html}
},

@inproceedings{glatard_implementation_2008,
	address = {Los Alamitos, {CA}, {USA}},
	title = {Implementation of Turing Machines with the Scufl Data-Flow Language},
	volume = {0},
	isbn = {978-0-7695-3156-4},
	doi = {http://doi.ieeecomputersociety.org/10.1109/CCGRID.2008.52},
	abstract = {In this paper, the expressiveness of the simple Scufl data-flow language is studied by showing how it can be used to implement Turing machines. To do that, several non trivial Scufl patterns such as self-looping or sub-workflows are required and we precisely explicit them. The main result of this work is to show how a complex workflow can be implemented using a very simple data-flow language. Beyond that, it shows that Scufl is a Turing complete language, given some restrictions that we discuss.},
	booktitle = {Cluster Computing and the Grid, {IEEE} International Symposium on},
	publisher = {{IEEE} Computer Society},
	author = {Glatard, Tristan and Montagnat, Johan},
	year = {2008},
	keywords = {scientific workflow, scufl, scufl language, turing machines, workflow},
	pages = {663--668},
	annote = {Complete {PDF} document was either not available or accessible. Please make sure you're logged in to the digital library to retrieve the complete {PDF} document.},
	file = {glatard-montagnat:2008.pdf:/home/pvh/Documents/Papers/Workflows/glatard-montagnat:2008.pdf:application/pdf;Implementation of Turing Machines with the Scufl Data-Flow Language:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/IXBTVZ8G/CCGRID.2008.html:text/html}
},

@article{oinn_delivering_2004,
	series = {{WWW} Alt. '04},
	title = {Delivering web service coordination capability to users},
	location = {New York, {NY}, {USA}},
	doi = {10.1145/1013367.1013514},
	abstract = {As web service technology matures there is growing interest in exploiting workflow techniques to coordinate web services. Bioinformaticians are a user community who combine web resources to perform in silico experiments. These users are scientists and not information technology experts they require workflow solutions that have a low cost of entry for service users and providers. Problems satisfying these requirements with current techniques led to the development of the Simple conceptual unified flow language {(Scufl).} Scufl is supported by the Freefluo enactment engine [1], and the Taverna editing workbench [3]. The extensibility of Scufl, supported by these tools, means that workflows coordinating web services can be matched to how users view their problems. The Taverna workbench exploits the web to keep Scufl simple by retrieving detail from {URIs} when required, and by scavenging the web for services. Scufl and its tools are not bioinformatics specific. They can be exploited by other communities who require user-driven composition and execution of workflows coordinating web resources.},
	journal = {Proceedings of the 13th international World Wide Web conference on Alternate track papers \& posters},
	author = {Oinn, Tom and Addis, Matthew and Ferris, Justin and Marvin, Darren and Greenwood, Mark and Goble, Carole and Wipat, Anil and Li, Peter and Carver, Tim},
	year = {2004},
	note = {{ACM} {ID:} 1013514},
	keywords = {bioinformatics, design, e-Science, experimentation, integrated environments, languages, programmer workbench, scientific workflows, scufl, web programming, web service coordination, web services, workflow},
	pages = {438–439},
	file = {10.1.1.1.6550.pdf:/home/pvh/Documents/Papers/Workflows/10.1.1.1.6550.pdf:application/pdf;ACM Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/H6M82G9B/citation.html:text/html}
},

@article{stein_case_2010,
	title = {The case for cloud computing in genome informatics},
	volume = {11},
	issn = {1465-6906},
	url = {http://genomebiology.com/2010/11/5/207/abstract},
	doi = {10.1186/gb-2010-11-5-207},
	number = {5},
	urldate = {2011-03-29},
	journal = {Genome Biology},
	author = {Stein, Lincoln D},
	year = {2010},
	pages = {207},
	file = {Genome Biology | Abstract | The case for cloud computing in genome informatics:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/TCBP2N6G/abstract.html:text/html}
},

@article{kahn_future_2011,
	title = {On the Future of Genomic Data},
	volume = {331},
	url = {http://www.sciencemag.org/content/331/6018/728.abstract},
	doi = {10.1126/science.1197891},
	abstract = {Many of the challenges in genomics derive from the informatics needed to store and analyze the raw sequencing data that is available from highly multiplexed sequencing technologies. Because single week-long sequencing runs today can produce as much data as did entire genome centers a few years ago, the need to process terabytes of information has become de rigueur for many labs engaged in genomic research. The availability of deep (and large) genomic data sets raises concerns over information access, data security, and subject/patient privacy that must be addressed for the field to continue its rapid advances.},
	number = {6018},
	urldate = {2011-03-30},
	journal = {Science},
	author = {Kahn, Scott D.},
	month = feb,
	year = {2011},
	pages = {728 --729},
	file = {Science-2011-Kahn-728-9.pdf:/home/pvh/Documents/Papers/Workflows/Science-2011-Kahn-728-9.pdf:application/pdf;Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/SXGXA9NA/728.html:text/html}
},

@article{newman_myexperiment:_2009,
	title = {{myExperiment:} An ontology for e-Research},
	author = {Newman, D. and Bechhofer, S. and De Roure, D.},
	year = {2009},
	file = {iswc2009swasd_submission_9.pdf:/home/pvh/Documents/Papers/Workflows/iswc2009swasd_submission_9.pdf:application/pdf}
},

@article{de_roure_towards_2009,
	title = {Towards open science: the {myExperiment} approach},
	issn = {1532-0634},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {De Roure, D. and Goble, C. and Aleksejevs, S. and Bechhofer, S. and Bhagat, J. and Cruickshank, D. and Fisher, P. and Hull, D. and Michaelides, D. and Newman, D.},
	year = {2009},
	file = {CCPE09v8.pdf:/home/pvh/Documents/Papers/Workflows/CCPE09v8.pdf:application/pdf}
},

@article{de_roure_myexperiment_2009,
	title = {The {myExperiment} Open Repository for Scientific Workflows},
	author = {De Roure, D. and Goble, C. and Aleksejevs, S. and Bechhofer, S. and Bhagat, J. and Cruickshank, D. and Michaelides, D. and Newman, D.},
	year = {2009},
	file = {ORmyExpv3.pdf:/home/pvh/Documents/Papers/Workflows/ORmyExpv3.pdf:application/pdf}
},

@inproceedings{de_roure_myexperiment:_2009,
	address = {Austin, Texas, {USA}},
	title = {{myExperiment:} A Web 2.0 Virtual Research Environment for Research using Computation and Services},
	url = {http://eprints.ecs.soton.ac.uk/17607/},
	abstract = {{myExperiment} is a social web site for the born-digital items arising in contemporary research practice, in particular scientific workflows and Research Objects. {myExperiment} can be seen from many perspectives: as a Virtual Research Environment which majors on social sharing, as {“Facebook} for scientists” but without the implicit openness which is actually a deterrent to scientists, as a second generation digital library which combines a  repository with a place for conducting in silico research, or as the foundation of the future e-Laboratory.},
	booktitle = {Workshop On Integrating Digital Library Content with Computational Tools and Services at {JCDL} 2009},
	author = {De Roure, David and Goble, Carole},
	year = {2009},
	file = {SGDLmyExp.pdf:/home/pvh/Documents/Papers/Workflows/SGDLmyExp.pdf:application/pdf}
},

@article{foster_anatomy_2001,
	title = {The Anatomy of the Grid: Enabling Scalable Virtual Organizations},
	volume = {15},
	shorttitle = {The Anatomy of the Grid},
	url = {http://hpc.sagepub.com/content/15/3/200.abstract},
	doi = {10.1177/109434200101500302},
	abstract = {{“Grid”} computing has emerged as an important new field, distinguished from conventional distributed computing by its focus on large-scale resource sharing, innovative applications, and, in some cases, high performance orientation. In this article, the authors define this new field. First, they review the {“Grid} problem,” which is defined as flexible, secure, coordinated resource sharing among dynamic collections of individuals, institutions, and resources—what is referred to as virtual organizations. In such settings, unique authentication, authorization, resource access, resource discovery, and other challenges are encountered. It is this class of problem that is addressed by Grid technologies. Next, the authors present an extensible and open Grid architecture, in which protocols, services, application programming interfaces, and software development kits are categorized according to their roles in enabling resource sharing. The authors describe requirements that they believe any such mechanisms must satisfy and discuss the importance of defining a compact set of intergrid protocols to enable interoperability among different Grid systems. Finally, the authors discuss how Grid technologies relate to other contemporary technologies, including enterprise integration, application service provider, storage service provider, and peer-to-peer computing. They maintain that Grid concepts and technologies complement and have much to contribute to these other approaches.},
	number = {3},
	urldate = {2011-06-01},
	journal = {International Journal of High Performance Computing Applications},
	author = {Foster, Ian and Kesselman, Carl and Tuecke, Steven},
	year = {2001},
	keywords = {grid, Grid computing},
	pages = {200 --222},
	file = {Full Text PDF:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/ZAQEVKQ8/Foster et al. - 2001 - The Anatomy of the Grid Enabling Scalable Virtual.pdf:application/pdf;Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/K4XIIEV3/200.html:text/html}
},

@misc{galaxy_team_galaxy_2011,
	title = {galaxy / galaxy-central / wiki / {Config/ProductionServer} – Bitbucket},
	url = {https://bitbucket.org/galaxy/galaxy-central/wiki/Config/ProductionServer},
	urldate = {2011-06-01},
	author = {Galaxy Team},
	month = jun,
	year = {2011},
	howpublished = {https://bitbucket.org/galaxy/galaxy-{central/wiki/Config/ProductionServer}},
	file = {galaxy / galaxy-central / wiki / Config/ProductionServer – Bitbucket:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/89NS3PSC/ProductionServer.html:text/html}
},

@article{cardoso_quality_2004,
	title = {Quality of service for workflows and web service processes},
	volume = {1},
	issn = {1570-8268},
	url = {http://www.sciencedirect.com/science/article/pii/S157082680400006X},
	doi = {doi: 10.1016/j.websem.2004.03.001},
	number = {3},
	journal = {Web Semantics: Science, Services and Agents on the World Wide Web},
	author = {Cardoso, Jorge and Sheth, Amit and Miller, John and Arnold, Jonathan and Kochut, Krys},
	month = apr,
	year = {2004},
	keywords = {Business process management, Predictive quality of service, Web process quality of service, Web service quality of service, Workflow management systems, Workflow quality of service},
	pages = {281--308},
	file = {10.1.1.92.710.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/JV8E6CEV/10.1.1.92.710.pdf:application/pdf}
},

@article{lovas_workflow_2004,
	title = {Workflow Support for Complex Grid Applications},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.93.5372},
	journal = {{INTEGRATED} {AND} {PORTAL} {SOLUTIONS}, {PROCEEDINGS} {OF} {2ND} {EUROPEAN} {ACROSS} {GRIDS} {CONFERENCE}},
	author = {Lovas, R. and Dózsa, G. and Kacsuk, P. and Podhorszki, N. and Drótos, D.},
	year = {2004},
	file = {10.1.1.93.5372.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/9XPFNIEG/10.1.1.93.5372.pdf:application/pdf}
},

@inproceedings{mcgough_workflow_2004,
	title = {Workflow enactment in {ICENI}},
	booktitle = {{UK} e-Science All Hands Meeting},
	publisher = {Citeseer},
	author = {{McGough}, S. and Young, L. and Afzal, A. and Newhouse, S. and Darlington, J.},
	year = {2004},
	pages = {894–900},
	file = {10.1.1.137.4143.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/PXPGU8KU/10.1.1.137.4143.pdf:application/pdf}
},

@article{van_der_aalst_workflow_2003,
	title = {Workflow patterns},
	volume = {14},
	number = {1},
	journal = {Distributed and parallel databases},
	author = {van Der Aalst, W. {M.P} and Ter Hofstede, A. {H.M} and Kiepuszewski, B. and Barros, A. P},
	year = {2003},
	pages = {5--51},
	file = {9950.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/PBDCFPJS/9950.pdf:application/pdf}
},

@inproceedings{altintas_kepler:_2004,
	title = {Kepler: an extensible system for design and execution of scientific workflows},
	isbn = {0-7695-2146-0},
	shorttitle = {Kepler},
	doi = {10.1109/SSDM.2004.1311241},
	abstract = {Most scientists conduct analyses and run models in several different software and hardware environments, mentally coordinating the export and import of data from one environment to another. The Kepler scientific workflow system provides domain scientists with an easy-to-use yet powerful system for capturing scientific workflows {(SWFs).} {SWFs} are a formalization of the ad-hoc process that a scientist may go through to get from raw data to publishable results. Kepler attempts to streamline the workflow creation and execution process so that scientists can design, execute, monitor, re-run, and communicate analytical procedures repeatedly with minimal effort. Kepler is unique in that it seamlessly combines high-level workflow design with execution and runtime interaction, access to local and remote data, and local and remote service invocation. {SWFs} are superficially similar to business process workflows but have several challenges not present in the business workflow scenario. For example, they often operate on large, complex and heterogeneous data, can be computationally intensive and produce complex derived data products that may be archived for use in reparameterized runs or other workflows. Moreover, unlike business workflows, {SWFs} are often dataflow-oriented as witnessed by a number of recent academic systems (e.g., {DiscoveryNet}, Taverna and Triana) and commercial systems {(Scitegic/Pipeline-Pilot}, Inforsense). In a sense, {SWFs} are often closer to signal-processing and data streaming applications than they are to control-oriented business workflow applications.},
	language = {English},
	booktitle = {16th International Conference on Scientific and Statistical Database Management, 2004. Proceedings},
	publisher = {{IEEE}},
	author = {Altintas, I. and Berkley, C. and Jaeger, E. and Jones, M. and Ludascher, B. and Mock, S.},
	month = jun,
	year = {2004},
	keywords = {Biological system modeling, Business, complex data, data access, Database Management Systems, data export, data flow computing, data handling, data import, data products, data streaming, {DiscoveryNet}, extensible system, hardware environment, heterogeneous data, high-level workflow design, Inforsense, Java, Kepler scientific workflow system, large data, Plugs, Power system modeling, Prototypes, Runtime, runtime interaction, scientific analysis, scientific information systems, scientific workflow design, scientific workflow execution, {Scitegic/Pipeline-Pilot}, service invocation, software environment, Supercomputers, Taverna, Triana, web services, workflow creation, workflow management software, Yarn},
	pages = {423-- 424},
	file = {ssdbm04-kepler.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/WZST2D4C/ssdbm04-kepler.pdf:application/pdf}
},

@article{deelman_workflows_2009,
	title = {Workflows and e-Science: An overview of workflow system features and capabilities},
	volume = {25},
	issn = {0167-{739X}},
	shorttitle = {Workflows and e-Science},
	url = {http://www.sciencedirect.com/science/article/pii/S0167739X08000861},
	doi = {10.1016/j.future.2008.06.012},
	abstract = {Scientific workflow systems have become a necessary tool for many applications, enabling the composition and execution of complex analysis on distributed resources. Today there are many workflow systems, often with overlapping functionality. A key issue for potential users of workflow systems is the need to be able to compare the capabilities of the various available tools. There can be confusion about system functionality and the tools are often selected without a proper functional analysis. In this paper we extract a taxonomy of features from the way scientists make use of existing workflow systems and we illustrate this feature set by providing some examples taken from existing workflow systems. The taxonomy provides end users with a mechanism by which they can assess the suitability of workflow in general and how they might use these features to make an informed choice about which workflow system would be a good choice for their particular application.},
	number = {5},
	urldate = {2011-12-05},
	journal = {Future Generation Computer Systems},
	author = {Deelman, Ewa and Gannon, Dennis and Shields, Matthew and Taylor, Ian},
	month = may,
	year = {2009},
	keywords = {Automation of scientific processes, Computation, Cyberinfrastructure, Distributed computing, Distributed systems, e-Science, Grid computing, scientific workflow, web services},
	pages = {528--540},
	file = {10.1.1.165.9173.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/STZ3J55M/10.1.1.165.9173.pdf:application/pdf;ScienceDirect Full Text PDF:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/6Z4VZUTG/Deelman et al. - 2009 - Workflows and e-Science An overview of workflow s:;ScienceDirect Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/TPD3Q95T/Deelman et al. - 2009 - Workflows and e-Science An overview of workflow s:}
},

@inproceedings{kandaswamy_fault_2008,
	title = {Fault Tolerance and Recovery of Scientific Workflows on Computational Grids},
	isbn = {978-0-7695-3156-4},
	doi = {10.1109/CCGRID.2008.79},
	abstract = {In this paper, we describe the design and implementation of two mechanisms for fault-tolerance and recovery for complex scientific workflows on computational grids. We present our algorithms for over-provisioning and migration, which are our primary strategies for fault-tolerance. We consider application performance models, resource reliability models, network latency and bandwidth and queue wait times for batch-queues on compute resources for determining the correct fault-tolerance strategy. Our goal is to balance reliability and performance in the presence of soft real-time constraints like deadlines and expected success probabilities, and to do it in a way that is transparent to scientists. We have evaluated our strategies by developing a Fault-Tolerance and Recovery {(FTR)} service and deploying it as a part of the Linked Environments for Atmospheric Discovery {(LEAD)} production infrastructure. Results from real usage scenarios in {LEAD} show that the failure rate of individual steps in workflows decreases from about 30\% to 5\% by using our fault-tolerance strategies.},
	language = {English},
	booktitle = {8th {IEEE} International Symposium on Cluster Computing and the Grid, 2008. {CCGRID} '08},
	publisher = {{IEEE}},
	author = {Kandaswamy, G. and Mandal, A. and Reed, D. A},
	month = may,
	year = {2008},
	keywords = {application performance model, Computational grids, Computer networks, Control systems, Fault tolerance, fault tolerance and recovery, fault tolerance-recovery service, Fault tolerant systems, Grid computing, Linked Environments for Atmospheric Discovery, Middleware, natural sciences computing, network latency, Probability, Processor scheduling, queue wait time, real-time constraint, resilient scientific workflows, resource reliability model, scheduling, scientific workflow, Software maintenance, {USA} Councils, Weather forecasting},
	pages = {777--782},
	file = {resilience08-5.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/G3T6V73S/resilience08-5.pdf:application/pdf}
},

@misc{_drmaa_????,
	title = {{DRMAA} 1.0 Grid Recommendation},
	url = {http://www.ogf.org/documents/GFD.133.pdf},
	abstract = {This document is an {OGF} Grid Recommendation, and acts as the normative base for other {DRMAA} specifications. Achieving Grid Recommendation status signifies that the {DRMAA} is being adopted within the distributed computing community. The standard had to pass several hurdles (as documented in {OGF} document {GFD.1)}, including the demonstration of interoperable implementations and the approval by the {GFSG.}},
	file = {GFD.133.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/MGUBHJ8U/GFD.133.pdf:application/pdf}
},

@misc{_drmaa_????-1,
	title = {{DRMAA} Python Language Binding {(GFD.143)}},
	url = {http://www.ogf.org/documents/GFD.143.pdf},
	abstract = {This is the {DRMAA} language binding for the Python programming language, based on {GFD.130.}},
	file = {GFD.143.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/JK8EZUM5/GFD.143.pdf:application/pdf}
},

@book{_eighth_1986,
	title = {Eighth Annual Conference of the Cognitive Science Society},
	isbn = {9780898598643},
	language = {en},
	publisher = {Routledge},
	year = {1986},
	keywords = {{ARTIFICIAL} intelligence, Cognition, Computer-assisted instruction, Intellect, Knowledge, Theory of, Psychology / Cognitive Psychology, Science / General}
},

@incollection{jordan_attractor_1986,
	title = {Attractor dynamics and parallelism in a connectionist sequential machine},
	isbn = {9780898598643},
	booktitle = {Proceedings of the Eighth Annual Conference of the Cognitive Science Society},
	publisher = {Routledge},
	author = {Jordan, Michael I.},
	year = {1986},
	file = {Jordan86.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/6N7VBV65/Jordan86.pdf:application/pdf}
},

@article{schadt_computational_2010,
	title = {Computational solutions to large-scale data management and analysis},
	volume = {11},
	number = {9},
	journal = {Nature Reviews Genetics},
	author = {Schadt, E. E and Linderman, M. D and Sorenson, J. and Lee, L. and Nolan, G. P},
	year = {2010},
	pages = {647–657},
	file = {52954978.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/HQF7AET2/52954978.pdf:application/pdf}
},

@article{stein_towards_2008,
	title = {Towards a cyberinfrastructure for the biological sciences: progress, visions and challenges},
	volume = {9},
	shorttitle = {Towards a cyberinfrastructure for the biological sciences},
	number = {9},
	journal = {Nature Reviews Genetics},
	author = {Stein, {L.D.}},
	year = {2008},
	pages = {678–688},
	file = {cyberinsfrastructuresteinnature2008.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/DXT6E9K3/cyberinsfrastructuresteinnature2008.pdf:application/pdf}
},

@article{neron_mobyle:_2009,
	title = {Mobyle: A New Full Web Bioinformatics Framework},
	volume = {25},
	issn = {1367-4803, 1460-2059},
	shorttitle = {Mobyle},
	url = {http://bioinformatics.oxfordjournals.org/content/25/22/3005},
	doi = {10.1093/bioinformatics/btp493},
	abstract = {Motivation: For the biologist, running bioinformatics analyses involves a time-consuming management of data and tools. Users need support to organize their work, retrieve parameters and reproduce their analyses. They also need to be able to combine their analytic tools using a safe data flow software mechanism. Finally, given that scientific tools can be difficult to install, it is particularly helpful for biologists to be able to use these tools through a web user interface. However, providing a web interface for a set of tools raises the problem that a single web portal cannot offer all the existing and possible services: it is the user, again, who has to cope with data copy among a number of different services. A framework enabling portal administrators to build a network of cooperating services would therefore clearly be beneficial.
Results: We have designed a system, Mobyle, to provide a flexible and usable Web environment for defining and running bioinformatics analyses. It embeds simple yet powerful data management features that allow the user to reproduce analyses and to combine tools using a hierarchical typing system. Mobyle offers invocation of services distributed over remote Mobyle servers, thus enabling a federated network of curated bioinformatics portals without the user having to learn complex concepts or to install sophisticated software. While being focused on the end user, the Mobyle system also addresses the need, for the bioinfomatician, to automate remote services execution: {PlayMOBY} is a companion tool that automates the publication of {BioMOBY} web services, using Mobyle program definitions.
Availability: The Mobyle system is distributed under the terms of the {GNU} {GPLv2} on the project web site (http://bioweb2.pasteur.fr/projects/mobyle/). It is already deployed on three servers: http://mobyle.pasteur.fr, http://mobyle.rpbs.univ-paris-diderot.fr and http://lipm-{bioinfo.toulouse.inra.fr/Mobyle.} The {PlayMOBY} companion is distributed under the terms of the {CeCILL} license, and is available at http://lipm-{bioinfo.toulouse.inra.fr/biomoby/PlayMOBY/.}
Contact: mobyle-support@pasteur.fr; mobyle-support@rpbs.univ-paris-diderot.fr; letondal@pasteur.fr
Supplementary {information:Supplementary} data are available at Bioinformatics online.},
	language = {en},
	number = {22},
	urldate = {2012-03-23},
	journal = {Bioinformatics},
	author = {Néron, Bertrand and Ménager, Hervé and Maufrais, Corinne and Joly, Nicolas and Maupetit, Julien and Letort, Sébastien and Carrere, Sébastien and Tuffery, Pierre and Letondal, Catherine},
	month = nov,
	year = {2009},
	pages = {3005--3011},
	file = {Full Text PDF:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/9W49RIZM/Néron et al. - 2009 - Mobyle A New Full Web Bioinformatics Framework.pdf:application/pdf;Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/NM3BGAEG/3005.html:text/html}
},

@article{antezana_biological_2009,
	title = {Biological knowledge management: the emerging role of the Semantic Web technologies},
	volume = {10},
	shorttitle = {Biological knowledge management},
	number = {4},
	journal = {Briefings in bioinformatics},
	author = {Antezana, E. and Kuiper, M. and Mironov, V.},
	year = {2009},
	pages = {392–407},
	file = {392.full.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/K2Z48U4J/392.full.pdf:application/pdf}
},

@article{addo-quaye_cleaveland:_2009,
	title = {{CleaveLand:} a pipeline for using degradome data to find cleaved small {RNA} targets},
	volume = {25},
	shorttitle = {{CleaveLand}},
	number = {1},
	journal = {Bioinformatics},
	author = {Addo-Quaye, C. and Miller, W. and Axtell, {M.J.}},
	year = {2009},
	pages = {130–131},
	file = {130.full.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/TZT74T53/130.full.pdf:application/pdf}
},

@article{gibson_data_2009,
	title = {The data playground: An intuitive workflow specification environment},
	volume = {25},
	issn = {0167-{739X}},
	shorttitle = {The data playground},
	url = {http://www.sciencedirect.com/science/article/pii/S0167739X0800157X},
	doi = {10.1016/j.future.2008.09.009},
	abstract = {Workflows systems are steadily finding their way into the work practices of scientists. This is particularly true in the in silico science of bioinformatics, where biological data can be processed by Web Services. In this paper, we investigate the potential of evolving the users’ interaction with workflow environments so that it more closely relates to the mode in which their day to day work is carried out. We present the Data Playground, an environment designed to encourage the uptake of workflow systems in bioinformatics through more intuitive interaction by focusing the user on their data, rather than on the processes. We implement a prototype plug-in for the Taverna workflow environment and show how this can promote the creation of workflow fragments by automatically converting the users’ interactions with data and Web Services into a more conventional workflow specification.},
	number = {4},
	urldate = {2012-03-23},
	journal = {Future Generation Computer Systems},
	author = {Gibson, Andrew and Gamble, Matthew and Wolstencroft, Katy and Oinn, Tom and Goble, Carole and Belhajjame, Khalid and Missier, Paolo},
	month = apr,
	year = {2009},
	keywords = {bioinformatics, Data driven workflows, scientific workflows, Workflow design},
	pages = {453--459},
	file = {DataPlayground.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/EBWXRKUZ/DataPlayground.pdf:application/pdf;ScienceDirect Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/8TDQPB3T/Gibson et al. - 2009 - The data playground An intuitive workflow specifi.html:text/html}
},

@article{woollard_asking_2010,
	title = {Asking complex questions of the genome without programming},
	journal = {Methods Mol. Biol},
	author = {Woollard, P.},
	year = {2010},
	pages = {39–52},
	file = {Woollard.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/6I8SMZ4Z/Woollard.pdf:application/pdf}
},

@book{durbin_biological_1998,
	title = {Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids},
	isbn = {9780521629713},
	shorttitle = {Biological Sequence Analysis},
	abstract = {Probablistic models are becoming increasingly important in analyzing the huge amount of data being produced by large-scale {DNA-sequencing} efforts such as the Human Genome Project. For example, hidden Markov models are used for analyzing biological sequences, linguistic-grammar-based probabilistic models for identifying {RNA} secondary structure, and probabilistic evolutionary models for inferring phylogenies of sequences from different organisms. This book gives a unified, up-to-date and self-contained account, with a Bayesian slant, of such methods, and more generally to probabilistic methods of sequence analysis. Written by an interdisciplinary team of authors, it is accessible to molecular biologists, computer scientists, and mathematicians with no formal knowledge of the other fields, and at the same time presents the state of the art in this new and important field.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Durbin, Richard},
	month = apr,
	year = {1998},
	keywords = {Amino acid sequence, Amino acid sequence - Statistical methods, Amino acid sequence/ Statistical methods, Markov processes, Medical / General, Nucleotide sequence, Nucleotide sequence - Statistical methods, Nucleotide sequence/ Statistical methods, Numerical analysis, Probabilities, Science / Chemistry / Organic, Science / Life Sciences / Biochemistry, Science / Life Sciences / General, Science / Life Sciences / Genetics \& Genomics, Science / Life Sciences / Molecular Biology}
},

@inproceedings{han_taxonomy_1998,
	title = {A taxonomy of adaptive workflow management},
	booktitle = {Workshop of the 1998 {ACM} Conference on Computer Supported Cooperative Work},
	author = {Han, Y. and Sheth, A. and Bussler, C.},
	year = {1998},
	file = {CSCW98Workshop han-sheth-bussler.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/CSH83A5I/CSCW98Workshop han-sheth-bussler.pdf:application/pdf}
},

@article{castro_workflows_2005,
	title = {Workflows in bioinformatics: meta-analysis and prototype implementation of a workflow generator},
	volume = {6},
	shorttitle = {Workflows in bioinformatics},
	number = {1},
	journal = {{BMC} bioinformatics},
	author = {Castro, A. G. and Thoraval, S. and Garcia, L. and Ragan, M.},
	year = {2005},
	pages = {87},
	file = {1471-2105-6-87.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/QTF7CC5J/1471-2105-6-87.pdf:application/pdf}
},

@inproceedings{bausch_bioopera:_2002,
	title = {Bioopera: Cluster-aware computing},
	shorttitle = {Bioopera},
	booktitle = {Cluster Computing, 2002. Proceedings. 2002 {IEEE} International Conference on},
	author = {Bausch, W. and Pautasso, C. and Schaeppi, R. and Alonso, G.},
	year = {2002},
	pages = {99–106},
	file = {BioOpera.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/B8E2VKM9/BioOpera.pdf:application/pdf}
},

@article{stevens_classification_2001,
	title = {A Classification of Tasks in Bioinformatics},
	volume = {17},
	issn = {1367-4803, 1460-2059},
	url = {http://bioinformatics.oxfordjournals.org/content/17/2/180},
	doi = {10.1093/bioinformatics/17.2.180},
	abstract = {Motivation: This paper reports on a survey of bioinformatics tasks currently undertaken by working biologists. The aim was to find the range of tasks that need to be supported and the components needed to do this in a general query system. This enabled a set of evaluation criteria to be used to assess both the biology and mechanical nature of general query systems.
Results: A classification of the biological content of the tasks gathered offers a checklist for those tasks (and their specialisations) that should be offered in a general bioinformatics query system. This semantic analysis was contrasted with a syntactic analysis that revealed the small number of components required to describe all bioinformatics questions. Both the range of biological tasks and syntactic task components can be seen to provide a set of bioinformatics requirements for general query systems. These requirements were used to evaluate two bioinformatics query systems.
Contact: robert.stevens@cs.man.ac.uk
Supplementary information: The questionnaire, responses and their summaries may be found at http://img.cs.man.ac.uk/tambis/questionnaire/bio-queries.html},
	language = {en},
	number = {2},
	urldate = {2012-05-09},
	journal = {Bioinformatics},
	author = {Stevens, Robert and Goble, Carole and Baker, Patricia and Brass, Andy},
	month = feb,
	year = {2001},
	pages = {180--188},
	file = {Bioinformatics-2001-Stevens-180-8.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/AUKKZUFM/Bioinformatics-2001-Stevens-180-8.pdf:application/pdf;Full Text PDF:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/M8IETGVI/Stevens et al. - 2001 - A Classification of Tasks in Bioinformatics.pdf:application/pdf;Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/JMBXM3BK/180.html:text/html}
},

@inproceedings{yildiz_towards_2009,
	address = {New York, {NY}, {USA}},
	series = {{WORKS} '09},
	title = {Towards scientific workflow patterns},
	isbn = {978-1-60558-717-2},
	url = {http://doi.acm.org/10.1145/1645164.1645177},
	doi = {10.1145/1645164.1645177},
	abstract = {Scientific workflow management systems provide users with a set of design primitives for process modeling and execution features that have different semantics and capabilities comparing to traditional workflow management systems. The main limitation that prevents the democratization of scientific workflow management systems is the lack of appropriate guidelines and abstract constructs for the development of workflow models. This paper takes on the challenge of developing design patterns for scientific workflow modeling. We present the hybrid semantics of scientific workflow modeling that compose control and data dependencies. We discuss the appropriateness of standard modeling notations to scientific workflow modeling and present the basic scientific workflow patterns.},
	urldate = {2012-05-10},
	booktitle = {Proceedings of the 4th Workshop on Workflows in Support of Large-Scale Science},
	publisher = {{ACM}},
	author = {Yildiz, Ustun and Guabtni, Adnene and Ngu, Anne H. H.},
	year = {2009},
	keywords = {patterns, scientific workflow management systems, workflow modeling},
	pages = {13:1–13:10},
	file = {a13-yildiz.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/WA3H3PNZ/a13-yildiz.pdf:application/pdf}
},

@misc{_scientific_????,
	title = {Scientific workflow management systems and workflow patterns},
	url = {http://0-search.proquest.com.oasis.unisa.ac.za/pqdtft/docview/304987880/1369B4999426FC6CD8D/1?accountid=14648},
	urldate = {2012-05-10},
	howpublished = {http://0-{search.proquest.com.oasis.unisa.ac.za/pqdtft/docview/304987880/1369B4999426FC6CD8D/1?accountid=14648}},
	file = {pdf.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/EJ97T849/pdf.pdf:application/pdf;Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/2FI27246/login.html:text/html}
},

@misc{nekrutenko_introduction_2011,
	address = {Lunteren, Netherlands},
	title = {Introduction to Galaxy},
	url = {http://wiki.g2.bx.psu.edu/Events/GCC2011},
	urldate = {2012-06-12},
	author = {Nekrutenko, Anton},
	month = may,
	year = {2011},
	file = {IntroductionSession.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/SEVDI8X8/IntroductionSession.pdf:application/pdf}
},

@misc{the_galaxy_team_galaxy_2011,
	title = {Galaxy Tool Shed},
	url = {http://toolshed.g2.bx.psu.edu/},
	urldate = {2012-06-13},
	author = {{{The} Galaxy Team}},
	year = {2011},
	howpublished = {http://toolshed.g2.bx.psu.edu/},
	file = {Galaxy Tool Shed:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/T9Q7FXHJ/toolshed.g2.bx.psu.edu.html:text/html}
},

@inproceedings{fahringer_askalon:_2005,
	address = {Washington, {DC}, {USA}},
	series = {{GRID} '05},
	title = {{ASKALON:} A Grid Application Development and Computing Environment},
	isbn = {0-7803-9492-5},
	shorttitle = {{ASKALON}},
	url = {http://dx.doi.org/10.1109/GRID.2005.1542733},
	doi = {10.1109/GRID.2005.1542733},
	urldate = {2012-06-16},
	booktitle = {Proceedings of the 6th {IEEE/ACM} International Workshop on Grid Computing},
	publisher = {{IEEE} Computer Society},
	author = {Fahringer, T. and Prodan, R. and Duan, Rubing and Nerieri, F. and Podlipnig, S. and Qin, Jun and Siddiqui, M. and Truong, Hong-Linh and Villazon, A. and Wieczorek, M.},
	year = {2005},
	pages = {122–131},
	file = {grid2005.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/9TED7EUX/grid2005.pdf:application/pdf}
},

@inproceedings{piccinelli_service-oriented_2003,
	title = {Service-oriented workflow: the {DySCo} framework},
	shorttitle = {Service-oriented workflow},
	doi = {10.1109/EURMIC.2003.1231604},
	abstract = {Workflow is the most popular choice among businesses for capturing and managing their operational knowledge. The conceptual and technology framework built around workflow in the past decades provides the basis for service-oriented concepts to be introduced into business information systems. We present an extension to traditional workflow that enables Web services to be composed into business solutions. The model proposed is embedded in the {DySCo} (dynamic service composer), which is also presented.},
	booktitle = {Euromicro Conference, 2003. Proceedings. 29th},
	author = {Piccinelli, G. and Finkelstein, A. and Williams, {S.L.}},
	month = sep,
	year = {2003},
	keywords = {business information systems, dynamic service composer framework, formal specification, information systems, Internet, knowledge acquisition, knowledge management, operational knowledge capturing, service-oriented workflow, web services, workflow management software, workflow process formalisation},
	pages = {291 -- 297},
	file = {IEEE Xplore Abstract Record:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/QAAWHJND/articleDetails.html:text/html}
},

@incollection{altintas_framework_2005,
	series = {Lecture Notes in Computer Science},
	title = {A Framework for the Design and Reuse of Grid Workflows},
	volume = {3458},
	isbn = {978-3-540-25810-0},
	url = {http://www.springerlink.com/content/qj1umbeqn00a6493/abstract/},
	abstract = {Grid workflows can be seen as special scientific workflows involving high performance and/or high throughput computational tasks. Much work in grid workflows has focused on improving application performance through schedulers that optimize the use of computational resources and bandwidth. As high-end computing resources are becoming more of a commodity that is available to new scientific communities, there is an increasing need to also improve the design and reusability “performance” of scientific workflow systems. To this end, we are developing a framework that supports the design and reuse of grid workflows. Individual workflow components (e.g., for data movement, database querying, job scheduling, remote execution etc.) are abstracted into a set of generic, reusable tasks. Instantiations of these common tasks can be functionally equivalent atomic components (called actors ) or composite components (so-called composite actors or subworkflows ). In this way, a grid workflow designer does not have to commit to a particular Grid technology when developing a scientific workflow; instead different technologies (e.g. {GridFTP}, {SRB}, and scp ) can be used interchangeably and in concert. We illustrate the application of our framework using two real-world Grid workflows from different scientific domains, i.e., cheminformatics and bioinformatics, respectively.},
	urldate = {2012-06-15},
	booktitle = {Scientific Applications of Grid Computing},
	publisher = {Springer Berlin / Heidelberg},
	author = {Altintas, Ilkay and Birnbaum, Adam and Baldridge, Kim and Sudholt, Wibke and Miller, Mark and Amoreira, Celine and Potier, Yohann and Ludaescher, Bertram},
	editor = {Herrero, Pilar and Pérez, María and Robles, Víctor},
	year = {2005},
	keywords = {Computer Science},
	pages = {295--299},
	file = {SpringerLink Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/BVG7R8KK/abstract.html:text/html}
},

@misc{tartakovsky_cyber_2011,
	address = {Pretoria, South Africa},
	title = {Cyber Infrastructure Approach for the Next-generation Sequencing at the National Institute of Allergy and Infectious Diseases},
	url = {http://www.chpcconf.co.za/Presentations/2011_06_M%20Tartakovsky.pdf},
	urldate = {2012-06-15},
	author = {Tartakovsky, Michael},
	month = dec,
	year = {2011},
	file = {2011_06_M Tartakovsky.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/TNJD89IQ/2011_06_M Tartakovsky.pdf:application/pdf}
},

@misc{the_galaxy_team_admin/tools/tool_2011,
	title = {{Admin/Tools/Tool} Config Syntax - Galaxy Wiki},
	url = {http://wiki.g2.bx.psu.edu/Admin/Tools/Tool%20Config%20Syntax},
	urldate = {2012-06-15},
	author = {{{The} Galaxy Team}},
	year = {2011},
	howpublished = {{http://wiki.g2.bx.psu.edu/Admin/Tools/Tool\%20Config\%20Syntax}},
	file = {Admin/Tools/Tool Config Syntax - Galaxy Wiki:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/SVSWESH6/Tool Config Syntax.html:text/html}
},

@article{churches_programming_2006,
	title = {Programming scientific and distributed workflow with Triana services},
	volume = {18},
	copyright = {Copyright © 2005 John Wiley \& Sons, Ltd.},
	issn = {1532-0634},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/cpe.992/abstract},
	doi = {10.1002/cpe.992},
	abstract = {In this paper, we discuss a real-world application scenario that uses three distinct types of workflow within the Triana problem-solving environment: serial scientific workflow for the data processing of gravitational wave signals; job submission workflows that execute Triana services on a testbed; and monitoring workflows that examine and modify the behaviour of the executing application. We briefly describe the Triana distribution mechanisms and the underlying architectures that we can support. Our middleware independent abstraction layer, called the Grid Application Prototype {(GAP)}, enables us to advertise, discover and communicate with Web and peer-to-peer {(P2P)} services. We show how gravitational wave search algorithms have been implemented to distribute both the search computation and data across the European {GridLab} testbed, using a combination of Web services, Globus interaction and {P2P} infrastructures. Copyright © 2005 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {10},
	urldate = {2012-06-16},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Churches, David and Gombas, Gabor and Harrison, Andrew and Maassen, Jason and Robinson, Craig and Shields, Matthew and Taylor, Ian and Wang, Ian},
	year = {2006},
	keywords = {Distributed systems, Grid, peer-to-peer, Triana, workflow},
	pages = {1021–1037},
	file = {Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/GC6WMGT6/abstract.html:text/html}
},

@article{deelman_pegasus:_2005,
	title = {Pegasus: A framework for mapping complex scientific workflows onto distributed systems},
	volume = {13},
	number = {3},
	journal = {Scientific Programming},
	author = {Deelman, E. and Singh, G. and Su, M. H. and Blythe, J. and Gil, Y. and Kesselman, C. and Mehta, G. and Vahi, K. and Berriman, G. B. and Good, J.},
	year = {2005},
	pages = {219--237},
	file = {Sci.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/GTPQ6RE5/Sci.pdf:application/pdf}
},

@article{bowers_provenance_2008,
	title = {Provenance in collection-oriented scientific workflows},
	volume = {20},
	copyright = {Copyright © 2007 John Wiley \& Sons, Ltd.},
	issn = {1532-0634},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/cpe.1226/abstract},
	doi = {10.1002/cpe.1226},
	abstract = {We describe a provenance model tailored to scientific workflows based on the collection-oriented modeling and design paradigm. Our implementation within the Kepler scientific workflow system captures the dependencies of data and collection creation events on preexisting data and collections, and embeds these provenance records within the data stream. A provenance query engine operates on self-contained workflow traces representing serializations of the output data stream for particular workflow runs. We demonstrate this approach in our response to the first provenance challenge. Copyright © 2007 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {5},
	urldate = {2012-06-16},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Bowers, Shawn and {McPhillips}, Timothy M. and Ludäscher, Bertram},
	year = {2008},
	keywords = {collection-oriented scientific workflows, provenance, scientific data management},
	pages = {519–529},
	file = {Full Text PDF:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/PFTTTQRN/Bowers et al. - 2008 - Provenance in collection-oriented scientific workf.pdf:application/pdf;Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/X4FZQ568/full.html:text/html}
},

@inproceedings{bowers_enabling_2006,
	title = {Enabling {ScientificWorkflow} Reuse through Structured Composition of Dataflow and Control-Flow},
	doi = {10.1109/ICDEW.2006.55},
	abstract = {Data-centric scientific workflows are often modeled as dataflow process networks. The simplicity of the dataflow framework facilitates workflow design, analysis, and optimization. However, modeling "control-flow intensive" tasks using dataflow constructs often leads to overly complicated workflows that are hard to comprehend, reuse, and maintain. We describe a generic framework, based on scientific workflow templates and frames, for embedding control-flow intensive subtasks within dataflow process networks. This approach can seamlessly handle complex control-flow without sacrificing the benefits of dataflow. We illustrate our approach with a real-world scientific workflow from the astrophysics domain, requiring remote execution and file transfer in a semi-reliable environment. For such workflows, we also describe a 3-layered architecture based on frames and templates where the top-layer consists of an overall dataflow process network, the second layer consists of a tranducer template for modeling the desired control-flow behavior, and the bottom layer consists of frames inside the template that are specialized by embedding the desired component implementation. Our approach can enable scientific workflows that are more robust (faulttolerance strategies can be defined by control-flow driven transducer templates) and at the same time more reusable, since the embedding of frames and templates yields more structured and modular workflow designs.},
	booktitle = {Data Engineering Workshops, 2006. Proceedings. 22nd International Conference on},
	author = {Bowers, S. and Ludascher, B. and Ngu, A. H. H. and Critchlow, T.},
	year = {2006},
	pages = {70},
	file = {4-wf-reuse-2006.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/N5FAEDKP/4-wf-reuse-2006.pdf:application/pdf;IEEE Xplore Abstract Record:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/9TT9BRT4/login.html:text/html}
},

@inproceedings{wang_adapting_2009,
	address = {Washington, {DC}, {USA}},
	series = {{SERVICES} '09},
	title = {Adapting the Galaxy Bioinformatics Tool to Support Semantic Web Service Composition},
	isbn = {978-0-7695-3708-5},
	url = {http://dx.doi.org/10.1109/SERVICES-I.2009.114},
	doi = {10.1109/SERVICES-I.2009.114},
	abstract = {As the availability of Web services for the biological domain increases, the need emerges for a Web service composition designer that is easy for biologists to use. Our work focuses on providing biologists and bioinformaticians with an online, semantic Web service composition tool. We adapt a bioinformatics tool called Galaxy, to support semantic Web service composition. A semi-automatic approach for semantic Web service composition is implemented. An easy to use online interface is provided.},
	urldate = {2012-06-17},
	booktitle = {Proceedings of the 2009 Congress on Services - I},
	publisher = {{IEEE} Computer Society},
	author = {Wang, Rui and Brewer, Douglas and Shastri, Shefali and Swayampakula, Srikalyan and Miller, John A. and Kraemer, Eileen T. and Kissinger, Jessica C.},
	year = {2009},
	keywords = {bioinformatics, composition, semantics, web services, workflow},
	pages = {283–290},
	file = {paper.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/N9NZRWTJ/paper.pdf:application/pdf}
},

@incollection{meyer_parallelism_2005,
	series = {Lecture Notes in Computer Science},
	title = {Parallelism in Bioinformatics Workflows},
	volume = {3402},
	isbn = {978-3-540-25424-9},
	url = {http://www.springerlink.com/content/wwuewa4120yak2b1/abstract/},
	urldate = {2012-06-17},
	booktitle = {High Performance Computing for Computational Science - {VECPAR} 2004},
	publisher = {Springer Berlin / Heidelberg},
	author = {Meyer, Luiz and Rössle, Shaila and Bisch, Paulo and Mattoso, Marta},
	editor = {Daydé, Michel and Dongarra, Jack and Hernández, Vicente and Palma, José},
	year = {2005},
	keywords = {Computer Science},
	pages = {233--257},
	file = {SpringerLink Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/KIBV8ANZ/abstract.html:text/html}
},

@article{sadedin_bpipe:_2012,
	title = {Bpipe: A Tool for Running and Managing Bioinformatics Pipelines},
	issn = {1367-4803, 1460-2059},
	shorttitle = {Bpipe},
	url = {http://bioinformatics.oxfordjournals.org/content/early/2012/04/11/bioinformatics.bts167},
	doi = {10.1093/bioinformatics/bts167},
	abstract = {Summary: Bpipe is a simple, dedicated programming language for defining and executing bioinformatics pipelines. It specialises in enabling users to turn existing pipelines based on shell scripts or command line tools into highly flexible, adaptable and maintainable workflows with a minimum of effort. Bpipe ensures that pipelines execute in a controlled and repeatable fashion and keeps audit trails and logs to ensure that experimental results are reproducible. Requiring only Java as a dependency, Bpipe is fully self-contained and cross platform, making it very easy to adopt and deploy into existing environments.
Availability and Implementation: Bpipe is freely available from http://bpipe.org under a {BSD} License.
Contact: Simon Sadedin simon.sadedin@mcri.edu.au
Supplementary information: Further technical information is available at Bioinformatics online.},
	language = {en},
	urldate = {2012-06-17},
	journal = {Bioinformatics},
	author = {Sadedin, Simon P. and Pope, Bernard and Oshlack, Alicia},
	month = apr,
	year = {2012},
	file = {Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/B8IJKFCW/bioinformatics.bts167.html:text/html}
},

@inproceedings{xu_biosflow_2009,
	title = {{BiosFlow} - A bioinformatics workflow platform based on semantic web technology},
	doi = {10.1109/FBIE.2009.5405845},
	abstract = {To facilitate biologist's research, workflow technology provides a generic mechanism to integrate diverse types of resources, such as databases, software and services, which facilitate knowledge exchange within different divergent fields. In this paper, we developed a workflow-based platform for integrating bioinformatics applications and tools, namely {BiosFlow.} {BiosFlow} is a Semantic Web platform, which provides nucleotide, protein and multiple neural spike train data analysis.},
	booktitle = {{BioMedical} Information Engineering, 2009. {FBIE} 2009. International Conference on Future},
	author = {Xu, Qing-Wei and Huang, Yu},
	month = dec,
	year = {2009},
	keywords = {bioinformatics, bioinformatics workflow platform, {BiosFlow}, knowledge exchange, multiple neural spike train data analysis, nucleotide data analysis, protein data analysis, semantic Web, semantic Web technology, workflow technology},
	pages = {37 --40},
	file = {IEEE Xplore Abstract Record:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/CP73JQJF/login.html:text/html}
},

@article{orvis_ergatis:_2010,
	title = {Ergatis: A Web Interface and Scalable Software System for Bioinformatics Workflows},
	volume = {26},
	issn = {1367-4803, 1460-2059},
	shorttitle = {Ergatis},
	url = {http://bioinformatics.oxfordjournals.org/content/26/12/1488},
	doi = {10.1093/bioinformatics/btq167},
	abstract = {Motivation: The growth of sequence data has been accompanied by an increasing need to analyze data on distributed computer clusters. The use of these systems for routine analysis requires scalable and robust software for data management of large datasets. Software is also needed to simplify data management and make large-scale bioinformatics analysis accessible and reproducible to a wide class of target users.
Results: We have developed a workflow management system named Ergatis that enables users to build, execute and monitor pipelines for computational analysis of genomics data. Ergatis contains preconfigured components and template pipelines for a number of common bioinformatics tasks such as prokaryotic genome annotation and genome comparisons. Outputs from many of these components can be loaded into a Chado relational database. Ergatis was designed to be accessible to a broad class of users and provides a user friendly, web-based interface. Ergatis supports high-throughput batch processing on distributed compute clusters and has been used for data management in a number of genome annotation and comparative genomics projects.
Availability: Ergatis is an open-source project and is freely available at http://ergatis.sourceforge.net
Contact: jorvis@users.sourceforge.net},
	language = {en},
	number = {12},
	urldate = {2012-06-17},
	journal = {Bioinformatics},
	author = {Orvis, Joshua and Crabtree, Jonathan and Galens, Kevin and Gussman, Aaron and Inman, Jason M. and Lee, Eduardo and Nampally, Sreenath and Riley, David and Sundaram, Jaideep P. and Felix, Victor and Whitty, Brett and Mahurkar, Anup and Wortman, Jennifer and White, Owen and Angiuoli, Samuel V.},
	month = jun,
	year = {2010},
	pages = {1488--1492},
	file = {Full Text PDF:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/EF6P5F93/Orvis et al. - 2010 - Ergatis A Web Interface and Scalable Software Sys.pdf:application/pdf;Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/KZ57QAMH/1488.html:text/html}
},

@article{velasco_elizondo_catalogue_2010,
	title = {A catalogue of component connectors to support development with reuse},
	volume = {83},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/S0164121210000051},
	doi = {10.1016/j.jss.2010.01.008},
	abstract = {Component-based development is based on the idea of building software systems by composing pre-existing components. Connectors are the ‘glue’ for composing components. Therefore, it is important to consider connectors as first-class entities and provide adequate descriptions of them to facilitate their understanding and promote their reuse. We have defined a catalogue of component connectors to support the process of ‘development with reuse’. The categories and connector types in the catalogue were obtained through an analysis of the activities involved in this process as well as considering the syntax and semantics of a new component model.},
	number = {7},
	urldate = {2012-06-17},
	journal = {Journal of Systems and Software},
	author = {Velasco Elizondo, Perla and Lau, Kung-Kiu},
	month = jul,
	year = {2010},
	keywords = {Adaptation, composition, Connectors, Development with reuse, Software components},
	pages = {1165--1178},
	file = {jss10.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/BE4WPCWT/jss10.pdf:application/pdf;ScienceDirect Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/IVSVRQTG/S0164121210000051.html:text/html}
},

@misc{workflow_management_coalition_workflow_1999,
	title = {Workflow Management Coalition Workflow Standard - Interoperability Abstract Specification},
	url = {www.wfmc.org/.../WFMC-TC-1012-Ver-2.0b-Draft-Interoperability-...},
	abstract = {This document is an abstract specification, which defines the functionality required to support
interoperability between different workflow engines. Workflow product vendors should use this document
to understand the principles of how interoperability between workflow engines are effected using the
{WfMC} Standards. They should then refer to specific transport binding specifications for details of how
conformant implementations must work.
This document does not present a standard that can be implemented, instead it presents a series of principles
that must be followed to submit interoperability standards to the Workflow Management Coalition.
Interoperability implementations cannot claim compliance with this specification, they can only claim
compliance with a specific binding of this specification.},
	urldate = {2012-06-17},
	publisher = {Workflow Management Coalition},
	author = {{{Workflow} Management Coalition}},
	year = {1999},
	file = {Abstract Specification WFMC-TC-1012 (PDF) November '99 2.0a 208KB.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/IZ3NK6KA/Abstract Specification WFMC-TC-1012 (PDF) November '99 2.0a 208KB.pdf:application/pdf}
},

@misc{the_galaxy_team_galaxy_2012,
	title = {The Galaxy Project: Online bioinformatics analysis for everyone},
	url = {http://galaxy.psu.edu/},
	abstract = {Galaxy is an open, web-based platform for data intensive biomedical research. Whether on the free public server or your own instance, you can perform, reproduce, and share complete analyses.},
	urldate = {2012-06-21},
	author = {{{The} Galaxy Team}},
	year = {2012},
	howpublished = {http://galaxy.psu.edu/},
	file = {The Galaxy Project: Online bioinformatics analysis for everyone:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/E28NMISK/galaxy.psu.edu.html:text/html}
},

@article{hollingsworth_workflow_1995,
	title = {Workflow management coalition: The workflow reference model},
	number = {1.1},
	journal = {Document Number {TC00-1003}},
	author = {Hollingsworth, D.},
	year = {1995},
	file = {10.1.1.198.5206.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/R4KT6FIR/10.1.1.198.5206.pdf:application/pdf}
},

@inproceedings{hollingsworth_workflow_2004,
	title = {The Workflow Reference Model: 10 Years On},
	shorttitle = {The Workflow Reference Model},
	abstract = {Last year saw the 10th anniversary of the Workflow Reference Model. This short paper reassesses the relevance of the Model in the current context of Business Process Management. It discusses the principles behind the Model, its strengths and weakness and examines how it remains relevant to the industry},
	booktitle = {Fujitsu Services, {UK;} Technical Committee Chair of {WfMC}},
	author = {Hollingsworth, David and Services, Fujitsu and Kingdom, United},
	year = {2004},
	pages = {295–312},
	file = {Citeseer - Full Text PDF:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/BCWBPSM9/Hollingsworth et al. - 2004 - The Workflow Reference Model 10 Years On.pdf:application/pdf;Citeseer - Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/BUE8T98V/summary.html:text/html}
},

@inproceedings{yildiz_business_2009,
	title = {Business versus Scientific Workflows: A Comparative Study},
	shorttitle = {Business versus Scientific Workflows},
	doi = {10.1109/SERVICES-I.2009.60},
	abstract = {The need for design primitives for scientific workflows has steadily increased over the years and, actually, has become more pronounced in recent years, with the employment of user-friendly scientific workflow management systems. In this paper, we conduct a comparative study between business and scientific workflows initiatives based on common workflow patterns found in business workflow. This study demonstrates some precise differences and identifies some key scientific workflow patterns that can be used in dataflow oriented scientific workflow systems without compromising the data-oriented modeling in scientific workflow.},
	booktitle = {Services - I, 2009 World Conference on},
	author = {Yildiz, U. and Guabtni, A. and Ngu, {A.H.H.}},
	month = jul,
	year = {2009},
	keywords = {business workflows, data flow computing, dataflow oriented scientific workflow systems, data-oriented modeling, design primitives, natural sciences computing, user-friendly scientific workflow management systems, workflow management software},
	pages = {340 --343},
	file = {CSE-2009-3.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/GQCA469D/CSE-2009-3.pdf:application/pdf;IEEE Xplore Abstract Record:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/4KJZ9PG3/login.html:text/html}
},

@inproceedings{abbott_experiences_1994,
	address = {New York, {NY}, {USA}},
	series = {{CSCW} '94},
	title = {Experiences with workflow management: issues for the next generation},
	isbn = {0-89791-689-1},
	shorttitle = {Experiences with workflow management},
	url = {http://doi.acm.org/10.1145/192844.192886},
	doi = {10.1145/192844.192886},
	abstract = {Workflow management is a technology that is considered strategically important by many businesses, and its market growth shows no signs of abating. It is, however, often viewed with skepticism by the research community, conjuring up visions of oppressed workers performing rigidly-defined tasks on an assembly line. Although the potential for abuse no doubt exists, workflow management can instead be used to help individuals manage their work and to provide a clear context for performing that work. A key challenge in the realization of this ideal is the reconciliation of workflow process models and software with the rich variety of activities and behaviors that comprise “real” work. Our experiences with the {InConcert} workflow management system are used as a basis for  outlining several issues that will need to be addressed in meeting this challenge. This is intended as an invitation to {CSCW} researchers to influence this important technology in a constructive manner by drawing on research and experience.},
	urldate = {2012-06-22},
	booktitle = {Proceedings of the 1994 {ACM} conference on Computer supported cooperative work},
	publisher = {{ACM}},
	author = {Abbott, Kenneth R. and Sarin, Sunil K.},
	year = {1994},
	keywords = {business process reengineering, workflow},
	pages = {113–120},
	file = {10.1.1.17.2685.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/G52VZWCZ/10.1.1.17.2685.pdf:application/pdf}
},

@article{mcphillips_scientific_2009,
	title = {Scientific workflow design for mere mortals},
	volume = {25},
	issn = {0167-{739X}},
	url = {http://www.sciencedirect.com/science/article/pii/S0167739X08000873},
	doi = {10.1016/j.future.2008.06.013},
	abstract = {Recent years have seen a dramatic increase in research and development of scientific workflow systems. These systems promise to make scientists more productive by automating data-driven and compute-intensive analyses. Despite many early achievements, the long-term success of scientific workflow technology critically depends on making these systems useable by “mere mortals”, i.e., scientists who have a very good idea of the analysis methods they wish to assemble, but who are neither software developers nor scripting-language experts. With these users in mind, we identify a set of desiderata for scientific workflow systems crucial for enabling scientists to model and design the workflows they wish to automate themselves. As a first step towards meeting these requirements, we also show how the collection-oriented modeling and design (comad) approach for scientific workflows, implemented within the Kepler system, can help provide these critical, design-oriented capabilities to scientists.},
	number = {5},
	urldate = {2012-06-23},
	journal = {Future Generation Computer Systems},
	author = {{McPhillips}, Timothy and Bowers, Shawn and Zinn, Daniel and Ludäscher, Bertram},
	month = may,
	year = {2009},
	keywords = {Automatic optimization, Collection, {COMAD}, Desiderata, provenance, Resilience, workflow},
	pages = {541--551},
	file = {1-s2.0-S0167739X08000873-main.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/V2MK675S/1-s2.0-S0167739X08000873-main.pdf:application/pdf;ScienceDirect Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/UAFGQNXC/S0167739X08000873.html:text/html}
},

@inproceedings{anand_efficient_2009,
	address = {New York, {NY}, {USA}},
	series = {{EDBT} '09},
	title = {Efficient provenance storage over nested data collections},
	isbn = {978-1-60558-422-5},
	url = {http://doi.acm.org/10.1145/1516360.1516470},
	doi = {10.1145/1516360.1516470},
	abstract = {Scientific workflow systems are increasingly used to automate complex data analyses, largely due to their benefits over traditional approaches for workflow design, optimization, and provenance recording. Many workflow systems employ a simple dependency model to represent the provenance of data produced by workflow runs. Although commonly adopted, this model does not capture explicit data dependencies introduced by "provenance-aware" processes, and it can lead to inefficient storage when workflow data is complex or structured. We present a provenance model, extending the conventional approach, that supports (i) explicit data dependencies and (ii) nested data collections. Our model adopts techniques from reference-based {XML} versioning, adding annotations for process and data dependencies. We present strategies and reduction techniques to store immediate and transitive provenance information within our model, and examine trade-offs among update time, storage size, and query response time. We evaluate our approach on real-world and synthetic workflow execution traces, demonstrating significant reductions in storage size, while also reducing the time required to store and query provenance information.},
	urldate = {2012-06-23},
	booktitle = {Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology},
	publisher = {{ACM}},
	author = {Anand, Manish Kumar and Bowers, Shawn and {McPhillips}, Timothy and Ludäscher, Bertram},
	year = {2009},
	pages = {958–969}
},

@incollection{mcphillips_collection-oriented_2006,
	series = {Lecture Notes in Computer Science},
	title = {Collection-Oriented Scientific Workflows for Integrating and Analyzing Biological Data},
	volume = {4075},
	isbn = {978-3-540-36593-8},
	url = {http://www.springerlink.com/content/9k88463171h36563/abstract/},
	urldate = {2012-06-23},
	booktitle = {Data Integration in the Life Sciences},
	publisher = {Springer Berlin / Heidelberg},
	author = {{McPhillips}, Timothy and Bowers, Shawn and Ludäscher, Bertram},
	editor = {Leser, Ulf and Naumann, Felix and Eckman, Barbara},
	year = {2006},
	keywords = {Computer Science},
	pages = {248--263},
	file = {dils06.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/IZXZ5DE5/dils06.pdf:application/pdf;SpringerLink Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/QDXTR857/abstract.html:text/html}
},

@article{de_roure_design_2009,
	title = {The design and realisation of the {myExperiment} Virtual Research Environment for social sharing of workflows},
	volume = {25},
	issn = {0167-{739X}},
	url = {http://www.sciencedirect.com/science/article/pii/S0167739X08000939},
	doi = {10.1016/j.future.2008.06.010},
	abstract = {In this paper we suggest that the full scientific potential of workflows will be achieved through mechanisms for sharing and collaboration, empowering scientists to spread their experimental protocols and to benefit from those of others. To facilitate this process we have designed and built the Experiment m y Virtual Research Environment for collaboration and sharing of workflows and experiments. In contrast to systems which simply make workflows available, Experiment m y provides mechanisms to support the sharing of workflows within and across multiple communities. It achieves this by adopting a social web approach which is tailored to the particular needs of the scientist. We present the motivation, design and realisation of Experiment m y .},
	number = {5},
	urldate = {2012-06-24},
	journal = {Future Generation Computer Systems},
	author = {De Roure, David and Goble, Carole and Stevens, Robert},
	month = may,
	year = {2009},
	keywords = {Collaborative computing, scientific workflow, taverna workflow workbench, virtual research environment, Workflow management},
	pages = {561--567},
	file = {1-s2.0-S0167739X08000939-main.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/GHZN5S6W/1-s2.0-S0167739X08000939-main.pdf:application/pdf;ScienceDirect Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/TQWXX2JQ/login.html:text/html}
},

@article{pignotti_enhancing_2011,
	title = {Enhancing workflow with a semantic description of scientific intent},
	volume = {9},
	issn = {1570-8268},
	url = {http://www.sciencedirect.com/science/article/pii/S1570826811000229},
	doi = {10.1016/j.websem.2011.05.001},
	abstract = {Scientists are becoming increasingly dependent upon resources available through the Internet including, for example, datasets and computational modelling services, which are changing the way they conduct their research activities. This paper investigates the use of workflow tools enhanced with semantics to facilitate the design, execution, analysis and interpretation of workflow experiments and exploratory studies. Current workflow technologies do not incorporate any representation of experimental constraints and goals, which we refer to in this paper as scientist’s intent. This paper proposes an abstract model of intent based on the Open Provenance Model {(OPM)} specification. To realise this model a framework based upon a number of Semantic Web technologies has been developed, including the {OWL} ontology language and the Semantic Web Rule Language {(SWRL).} Through the use of social simulation case studies the paper illustrates the benefits of using this framework in terms of workflow monitoring, workflow provenance and annotation of experimental results.},
	number = {2},
	urldate = {2012-06-24},
	journal = {Web Semantics: Science, Services and Agents on the World Wide Web},
	author = {Pignotti, Edoardo and Edwards, Peter and Gotts, Nick and Polhill, Gary},
	month = jul,
	year = {2011},
	keywords = {provenance, Rules, Scientist’s intent, Semantic Grid, workflow},
	pages = {222--244},
	file = {pignotti_jws_july2011.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/QFU7DW4F/pignotti_jws_july2011.pdf:application/pdf;ScienceDirect Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/5EUGPKQC/S1570826811000229.html:text/html}
},

@article{zhao_editorial:_2009,
	title = {Editorial: Special section on workflow systems and applications in e-Science},
	volume = {25},
	issn = {0167-{739X}},
	shorttitle = {Editorial},
	url = {http://dx.doi.org/10.1016/j.future.2008.10.011},
	doi = {10.1016/j.future.2008.10.011},
	number = {5},
	urldate = {2012-06-24},
	journal = {Future Gener. Comput. Syst.},
	author = {Zhao, Zhiming and Belloum, Adam and Bubak, Marian},
	month = may,
	year = {2009},
	pages = {525–527},
	file = {1-s2.0-S0167739X08001726-main.pdf:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/2EE28N5A/1-s2.0-S0167739X08001726-main.pdf:application/pdf}
},

@article{elmroth_three_2010,
	title = {Three fundamental dimensions of scientific workflow interoperability: Model of computation, language, and execution environment},
	volume = {26},
	issn = {0167-{739X}},
	shorttitle = {Three fundamental dimensions of scientific workflow interoperability},
	url = {http://www.sciencedirect.com/science/article/pii/S0167739X09001174},
	doi = {10.1016/j.future.2009.08.011},
	abstract = {We investigate interoperability aspects of scientific workflow systems and argue that the workflow execution environment, the model of computation {(MoC)}, and the workflow language form three dimensions that must be considered depending on the type of interoperability sought: at the activity, sub-workflow, or workflow levels. With a focus on the problems that affect interoperability, we illustrate how these issues are tackled by current scientific workflows as well as how similar problems have been addressed in related areas. Our long-term objective is to achieve (logical) interoperability between workflow systems operating under different {MoCs}, using distinct language features, and sharing activities running on different execution environments.},
	number = {2},
	urldate = {2012-06-24},
	journal = {Future Generation Computer Systems},
	author = {Elmroth, Erik and Hernández, Francisco and Tordsson, Johan},
	month = feb,
	year = {2010},
	keywords = {Grid interoperability, Model of computation, scientific workflows, Workflow interoperability, Workflow languages},
	pages = {245--256},
	file = {ScienceDirect Snapshot:/home/pvh/.mozilla/firefox/pp9q0w3a.default/zotero/storage/MFW9I69B/S0167739X09001174.html:text/html}
}